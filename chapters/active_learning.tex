\section{Introduction}
\label{sec:active_introduction}
This work was presented at the European Congress on Digital Pathology 2019 and published as part of its proceedings ~\citep{carse2019active}.

\subsection{Active Learning for Medical Image Analysis}
\label{subsec:active_for_medical_image_analysis}
Active learning is a type of machine learning that hypothesises that having a learning algorithm select the data that is used during training can reduce the amount of data needed for training~\citep{settles2012active}. Active learning is used within modern applications to reduce the quantity of data that needs to be annotated by selecting unannotated data to be annotated and added to the dataset used to train the model. Limiting the amount of data annotations needed can reduce annotation costs (which can be expensive when dealing with specialised data such as histopathology) and computation costs as the models can be trained with fewer data. In a pool-based scenario, the learning algorithm has access to a large pool of unannotated data. Over multiple iterations, the learning algorithm selects the most beneficial data from the pool to be annotated and added the training dataset, as shown in figure \ref{fig:pool_based_active_learning}. One of the main advantages of pool-based active machine learning for medical image analysis is its ability to reduce the amount of human labour required. Medical image analysis often involves manual annotation, which can be time-consuming and labour-intensive. By using pool-based active learning, the burden of annotation is greatly reduced, as the algorithm can identify the most informative samples and prioritize them for labelling.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\textwidth]{images/active_learning.png}
	\caption{Pool-based active learning framework.}
	\label{fig:pool_based_active_learning}
\end{figure}

Active learning algorithms utilize query strategies to select data for annotation. While some popular query strategies, such as uncertainty sampling, have been demonstrated to be effective on deep learning algorithms~\citep{gal2017deep}, the unique feature-representation learning process of deep learning algorithms can present challenges. Specifically, the selection of only difficult examples for training can lead to a lower-quality model due to the resulting features not being representative of the entire data distribution. This issue is illustrated by \cite{pop2018deep}, who demonstrate the occurrence of mode collapse when using a Bayesian uncertainty query strategy to train a convolutional neural network (CNN). To address these challenges, batch-aware query strategies that make use of clustering methods have been shown to be effective in deep learning environments\citep{sener2017active, zhdanov2019diverse, kirsch2019batchbald}. These strategies optimize the selection of batches of images for annotation rather than individual data points.

\subsection{Deep Active Learning for Digital Pathology}
\label{subsec:active_deep_learning}
To save computation time, it is common practice in digital pathology to use large patches from whole slide images when applying machine learning algorithms. These patches can be efficiently processed by deep learning algorithms like convolutional neural networks (CNNs), and do not require the entire slide image to be annotated. However, using patch-based methods with small patches for tasks such as nuclei detection and classification can be problematic when using active learning to query for annotation. This is because small patches are more time-consuming and labour-intensive to annotate, and may lack sufficient spatial context for accurate annotation, even for expert pathologists.

To improve annotation efficiency and reduce costs, this chapter proposes a modified active learning framework for selecting larger regions comprising multiple small patches for annotation. This modified framework was tested using various active learning query strategies on a nuclei detection and classification task using the CRCHistoPhenotypes dataset~\citep{sirinukunwattana2016locality}.



\section{Active Learning for Medical Images Review}
\label{sec:active_review}
Active Learning for Medical Images Review

\subsection{Pool Based Active Learning Method}
\label{subsec:active_pool_based}
Pool Based Active Learning Method 

\subsubsection{Traditional Machine Learning Query Strategies}
\textbf{Uncertainty sampling} is a method in active learning in which the learning algorithm focuses on data points it is most uncertain about to improve model performance. This can be measured using techniques such as entropy or distance to the decision boundary. It allows the algorithm to selectively request labels for the most informative data points, leading to more efficient and effective learning. It can be used in conjunction with other active learning strategies such as representative and diversity sampling.

\textbf{Query by committee} is a method in active learning in which a committee of multiple classifiers make predictions on unlabelled data. If their predictions are diverse or conflicting, the learning algorithm may request a label for that data point. Query by committee can help reduce overfitting and improve generalisation and can be used with other active learning strategies like uncertainty and representative sampling.

\textbf{Expected model change} is a method in active learning in which the learning algorithm estimates the change in overall performance after labelling a particular data point and prioritizes data points with the greatest expected impact. This allows the algorithm to focus on data points most likely to improve performance. It can be used with other active learning strategies like uncertainty and representative sampling.

\textbf{Expected error reduction} is a method in active learning in which the learning algorithm estimates the reduction in error rate after labelling a particular data point, and prioritizes data points with the greatest expected impact. This allows the algorithm to focus on data points most likely to improve performance. It can be used with other active learning strategies like uncertainty and representative sampling.

\textbf{Variance reduction} is a method in active learning in which the learning algorithm prioritizes data points that are expected to have the greatest impact on reducing the variance of the predictions. This can be achieved by calculating the variance of the predictions for a particular data point. It can be used with other active learning strategies like uncertainty and representative sampling.

\textbf{Density-weighted methods} for active learning involve selecting data samples based on the density of the samples in the feature space, to select samples that are underrepresented or less dense. These samples are likely to be more informative and valuable for the model to learn from, which can improve its performance and generalization. There are several ways to implement density-weighted methods, including using a density estimate or weighting samples based on their informativeness and density.


\subsubsection{Deep Learning Query Strategies}
Deep Learning Query Strategies

\subsection{Application of Active Learning for Medicine}
\label{subsec:active_applications}
Application of Active Learning for Medicine

\subsection{Active Learning for Annotation Efficiently}
\label{subsec:active_annotation_efficiently}
Active Learning for Annotation Efficiently




\section{Annotator Efficient Active Learning for Histopathology}
\label{sec:active_annotator_efficient}
Annotator Efficient Active Learning for Histopathology

\subsection{Problem Setting defined by Review}
\label{subsec:active_problem_settings}
Problem Setting defined by Review

\subsection{Region-Based Active Learning}
\label{subsec:active_region_based}
Patch-based methods are common within digital pathology and medical image analysis more generally. However, applying active learning to these methods can be tedious, especially in systems that use small patches. Small patches can be difficult to annotate in isolation. Even if their spatial visual context is provided to the annotator, continually having to reassess the context for each annotation can be inefficient and frustrating.  We propose a region-based alternative that requests annotations over larger regions containing multiple small patches. Working with larger regions eases the effort needed from the annotator and can lead to an improved annotation collection throughput. This alteration allows for a learning algorithm to be trained with the small patches and only treats the data as regions when querying the unannotated data.

\begin{algorithm}
	\caption{Region-based active learning}
	\label{alg:regionbased}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwProg{RegionQueryStrategy}{RegionQueryStrategy}{}{}
	\Input{
		$\theta \gets$ trained model,\\
		$\delta \gets$ prediction algorithm,\\
		$S \gets$ active learning query strategy,\\
		$U \gets$ set of unannotated data,\\
		$A \gets$ set of annotated data
	}
	\Output{
		$U' \gets$ updated set of unannotated data,\\
		$A' \gets$ updated set of annotated data
	}
	\RegionQueryStrategy{$\theta, \delta, S, U, A$}
	{
		\ForEach{region $r$ in $U$}{
			$P \gets$ ExtractPatches($r$) \hfill extract patches from region\\
			$O \gets \theta(\delta, P)$ \hfill predictions on extracted patches\\
			$O' \gets$ Average($O$) \hfill average the patch predictions\\
			$Y \gets$ Append($Y, O'$) \hfill append region average to array\\
		}
		$n \gets S(Y)$ \hfill selects region from list\\
		$U' \gets$ Remove($U, n$) \hfill removes selection from unannotated set\\
		$A' \gets$ Append($A, n$) \hfill appends the selection to annotated set\\
		\Return{$U', A'$}
	}
\end{algorithm}

The proposed query strategy makes a simple modification to how an existing query strategy works. An overview of this can be seen in Algorithm \ref{alg:regionbased} where \(S\) is an existing query strategy. This algorithm is called at the end of each active iteration once a model has been trained on the currently available annotations. It extracts all the patches from each unannotated region and makes predictions on each patch. These predictions are then averaged to create a prediction for the overall region. Once all the regions have predictions, these predictions can be used within an active learning query strategy. An example of this would be using entropy uncertainty sampling where an uncertainty value for each region would be calculated and sampled. However, this approach can also be applied to more complex query strategies such as core-set sampling, by solving the K-centre problem for the region predictions rather than feature representations for individual data points.



\section{Active Learning Experiments}
\label{sec:active_experiments}
This section details the datasets, training parameters, experimental setup, and results for the experiments with region-based active learning on a nuclei classification task from whole slide image patches. The code and full results used within this section can be found on the project GitHub repository\footnote{GitHub Repository: \url{github.com/jmcjacob/Patch-Active-Learning-Pathology}}.

\subsection{Dataset}
\label{subsec:active_dataset}
The publicly available CRCHistoPhenotypes~\citep{sirinukunwattana2016locality} dataset was used to evaluate region-based active learning as the dataset contained a large number of annotated nuclei from large patches extracted from whole slide images making it ideal for the use case for region based active learning. The dataset has also been used in several nuclei classification and detection studies and has also been used for active learning experiments~\citep{shao2018deep}. The dataset consists of 22,444 annotated nuclei from 100 500x500 non-overlapping patches from 10 whole slide H\&E images of colorectal adenocarcinomas from 9 different patients. Each nucleus has its coordinates and its corresponding classification (epithelial, inflammatory, fibroblast, and miscellaneous) annotated. There are 7,722 epithelial, 5,712 fibroblast, 6971 inflammatory and 2,039 miscellaneous annotated nuclei in the dataset. 2,500 images were produced by splitting the patches into 100x100 pixel regions. From these regions, each nucleus was extracted into a 30x30 patch that was used to train the CNN model. Augmentation was used during training by randomly applying Gaussian blurring and horizontal and vertical flipping.

\begin{figure}[t!]
	\centering
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\linewidth]{region1.jpg}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\linewidth]{region2.jpg}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\linewidth]{region3.jpg}
	\end{subfigure}
	\caption{Three example regions from the CRCHistoPhenotypes dataset~\cite{sirinukunwattana2016locality} with multiple nuclei that will be extracted into patches and augmented.}
	\label{fig:region_example}
\end{figure}

\subsection{Training Parameters}
\label{subsec:active_training}
This experiment used a simple CNN inspired by the architecture used in the nuclei classification benchmark for the CRCHistoPhenotypes dataset~\citep{sirinukunwattana2016locality}. It consisted of two convolutional layers, one with 36 4x4 filters and the other with 48 3x3 filters, both of which are followed by 2x2 max pooling layers. The convolutional layers were followed by two fully connected layers with 1200 neurons and 512 neurons respectively. This architecture is summarised in Table~\ref{tab:active_learning_cnn}. Each hidden layer used ReLU activation functions, and the two fully connected layers use dropout for regularisation~\citep{srivastava2014dropout} with a drop chance of 0.5. Dropout is used to train the model so the model can be used for Monte Carlo sampling after training.

\begin{table}[h]
	\caption{The Convolutional Neural Network architecture for nuclei classification used in the region-based active learning experiments.}
	\label{tab:active_learning_cnn}
	\centering
	\begin{tabular}{|c|c|c}
		\hline
		Type & \begin{tabular}[c]{@{}c@{}}Filter\\ Dimensions\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Input/Output\\ Dimensions\end{tabular}} \\ \hline
		I &  & \multicolumn{1}{c|}{30 x 30 x 3} \\ \hline
		C & 4 x 4 x 1 x 36 & \multicolumn{1}{c|}{26 x 26 x 36} \\ \hline
		M & 2 x 2 & \multicolumn{1}{c|}{12 x 12 x 36} \\ \hline
		C & 3 x 3 x 36 x 48 & \multicolumn{1}{c|}{10 x 10 x 48} \\ \hline
		M & 2 x 2 & \multicolumn{1}{c|}{5 x 5 x 48} \\ \hline
		F & 5 x 5 x 48 x 1200 & \multicolumn{1}{c|}{1 x 1200} \\ \hline
		F & 1 x 1 x 512 x 512 & \multicolumn{1}{c|}{1 x 512} \\ \hline
		F & 1 x 1 x 512 x 4 & \multicolumn{1}{c|}{1 x 4} \\ \hline
	\end{tabular}

\end{table}

During the active learning process, the training environment is constantly changing as the training data is increased. To keep up with this change the adaptive gradient decent algorithm Adadelta~\citep{zeiler2012adadelta} was chosen as it requires no manual tuning of the learning rate as it adapts to the gradients of the model. As the training environment is constantly changing and may have to deal with small quantities of annotated data models can easily overfit the training data, to avoid this early stopping is used. The method proposed by \cite{prechelt1998early} compares generalisation loss and training progression until it reaches a specified target $\frac{GL(t)}{P_k(t)} > \alpha$. Generalisation loss (Equation \ref{eq:generalization_loss}) is calculated by comparing the validation loss for each epoch \(L_{val}(t)\) against the minimum validation loss across all epochs. The training progression (Equation \ref{eq:generalization_loss}) value is calculated by analysing the training losses \(L_{tr}(t)\) over a batch of recent epochs of size \(k\).

\begin{equation}
	GL(t) = 100 \cdot \left ( \frac{L_{va}(t)}{\underset{t'\leq t}{min}L_{va}(t')} - 1 \right )
	\label{eq:generalization_loss}
\end{equation}
\begin{equation}
	P_k(t) = 1000 \cdot \left ( \frac{\sum_{t'=t-k+1}^{t}L_{tr}(t')}{k \cdot min^{t}_{t'=t-k+1}L_{tr}(t')} - 1\right )
	\label{eq:training_progression}
\end{equation}

\subsection{Experiment Setup}
\label{subsec:active_experiments}
Experiments testing the region-based active learning modification combined with a range of different query strategies. These query strategies included several basic methods used specifically to act as baselines compared to others built specifically for deep learning query algorithms. The baseline strategies chosen were random querying, least confident uncertainty, margin uncertainty and entropy uncertainty sampling. The deep learning-specific query strategies used were K-Centre sampling (using greedy approximation), core-set sampling~\citep{sener2017active} and Bayesian active learning by disagreement (BALD) using Monte Carlo dropout~\citep{gal2017deep}. These methods were chosen as they are state of the art for query strategies deep active learning methods, cost-effective active learning (CEAL)~\citep{wang2016cost} was considered for use in this study but due to its higher computational cost it was not used.

In each experiment, all the available data was initially treated as unannotated; two randomly selected regions were used to form the initial annotated training dataset. For each active iteration, two regions were selected from the unannotated pool of regions to be added to the training dataset, a randomly initialised model (using uniform Xavier initialisation~\citep{glorot2010understanding}) is then trained using the new dataset. This continued for 50 active iterations meaning that 102 regions out of 2,500 were used to form the final training set in each experiment. Each experimental setting was run five times with different seeds used to generate random elements (model weight initialisation and initial annotated patches).

\subsection{Results}
\label{subsec:active_results}
The query strategies were evaluated by testing the trained models after each active iteration on a single, unchanging test set. Table~\ref{tab:query_results} gives the test accuracy and cross-entropy loss after 50 iterations (averaged over five runs) for each of the tested query strategies. Notably, only K-Centre sampling achieved a higher average accuracy than a random sampling query strategy. The core-set sampling query strategy accuracy was very similar to that of random sampling. The other query strategies all performed worse than the random sampling baseline. Figures ~\ref{fig:active_learning_accuracy}, \ref{fig:active_learning_mean_class_accuracy} and \ref{fig:active_learning_loss} show the test accuracy, mean class accuracy and cross-entropy loss for the models trained with annotated data selected by each query strategy after each active iteration averaged over 5 runs. The figures also show the results of training a model with all the available regions annotated (1487 annotated training regions). For comparison, a fully supervised CNN trained on the entire dataset of annotated regions achieved an accuracy of 68.53\% and a cross-entropy loss of 1.111. Training using the K-Centre query strategy achieved an accuracy of 61.41\% and a cross-entropy loss of 1.137 using only 7\% of the annotated data.

\begin{table}
	\centering
	\caption{Test results for each query strategy after 50 active iterations.}
	\label{tab:query_results}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{c|ccccccc}
			\begin{tabular}[c]{@{}c@{}}Query\\ Strategy\end{tabular} & Random & \begin{tabular}[c]{@{}c@{}}Least \\ Confident\end{tabular} & Margin & Entropy & K-Centre & Core-Set & BALD \\ \hline
			Accuracy & 58.25 & 48.92 & 45.84 & 32.37 & 61.41 & 57.33 & 48.23 \\
			\begin{tabular}[c]{@{}c@{}}Mean Class \\ Accuracy\end{tabular} & 53.50 & 47.36 & 42.40 & 41.07 & 54.39 & 52.50 & 46.14 \\
			Loss & 1.154 & 1.243 & 1.268 & 1.39 & 1.123 & 1.157 & 1.247
		\end{tabular}%
	}
\end{table}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/active_learning_accuracy.png}
	\caption{Average test accuracy of trained models with different amounts of annotated regions selected with query strategises.}
	\label{fig:active_learning_accuracy}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/active_learning_mean_class_accuracy.png}
	\caption{Average test mean class accuracy of trained models with different amounts of annotated regions selected with query strategises.}
	\label{fig:active_learning_mean_class_accuracy}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/active_learning_loss.png}
	\caption{Average test loss of trained models with different amounts of annotated regions selected with query strategises.}
	\label{fig:active_learning_loss}
\end{figure}


\section{Conclusion}
\label{sec:active_conclusion}
This chapter proposed a mechanism for reducing annotator effort in deep active learning for patch-based nuclei classification systems. The results presented in Section \ref{subsec:active_results} demonstrate that when using deep learning models, traditional active learning query strategies performed poorly. Active learning methods tailored specifically for deep learning also performed poorly when compared to random baselines. This has been discussed within the field on active learning methods for deep learning \citep{ren2021survey} and indicates a need for more extensive active learning methods for deep learning. Reducing annotation overheads and thus the cost of developing deep learning systems for digital pathology and medical image analysis can allow those with less access to resources to work on a range of problems. Methods such as active learning have great potential, but further work is needed to achieve significant gains on tasks such as those presented. This led to investigating unsupervised learning to better use unannotated data and can work alongside active learning.
