\section{Neural Networks for Computer Vision}
\label{sec:ann_for_cv}
Neural Networks are biologically inspired information processing systems composed of interconnected processing elements known as neurons. Neural networks differ from a lot of traditional machine learning algorithms as they are able to jointly learn deep representation of raw input data and train a discriminating classifier for the representations. 


\subsection{Neural Network Basics}
\label{subsec:neural_network_basics}

\subsubsection{Neurons}

\begin{equation}
	y = f\left ( \left ( \sum_{i=0}^{I} w_i \cdot x_i \right ) + b \right )
	\label{eq:neuron}
\end{equation}

\begin{equation}
	f(x)=\left\{\begin{matrix}
		0 & for & x < 0 \\ 
		x & for & x \geq 0
	\end{matrix}\right.
	\label{eq:relu}
\end{equation}


\subsubsection{Loss Functions}

\begin{equation}
	L = -\frac{1}{N}\sum^N_{n=1}\left [ y_n\log a_n + (1-y_n)\log(1-a_n) \right ]
	\label{eq:cross_entropy}
\end{equation}


\subsubsection{Backpropagation}
Backpropagation utilises the chain rule to try and identify the 

\begin{equation}
	\frac{\partial L_{total}}{\partial w_{i,j}} = \frac{\partial L_{total}}{\partial out_j} \cdot \frac{\partial out_j}{\partial net_j} \cdot \frac{\partial net_j}{\partial w_{i,j}}
	\label{eq:backpropagation}
\end{equation}


\subsubsection{Gradient Descent}

\begin{equation}
	w_{i,j} \leftarrow w_{i,j} - \eta \cdot \frac{\partial L_{total}}{\partial w_{i,j}}
	\label{gradient_descent}
\end{equation}



\subsection{Convolutional Neural Networks}
\label{subsec:convolutional_neural_networks}



\subsection{Visual Transformers}
\label{subsec:visual_transformers}





\section{Active Learning}
\label{sec:active_learning}





\section{Unsupervised Representation Learning}
\label{sec:unsupervised_representation_learning}





\section{Selective Classification}
\label{sec:selective_classification}
