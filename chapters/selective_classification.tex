\section{Introduction}
\label{sec:selective_introduction}
\subsection{Robust Selective Classification for Skin\\Lesions}
\label{subsec:robust and selective}
The utilisation of automated image analysis for the assessment of skin lesions holds great promise in enhancing diagnostic accuracy and streamlining clinical workflows within the fields of dermatology and primary care. By employing lesion classifiers that generate class probability distributions, it becomes possible to estimate the associated costs of clinical decisions, such as referral recommendations, thereby facilitating informed decision making.

It is important to note that the costs resulting from misclassification are typically asymmetric, with a greater financial impact associated with falsely categorising a malignant lesion as benign compared to the reverse. To achieve optimal decision making, it is crucial that the predicted class probabilities are properly calibrated. Additionally, a clinically practical system should possess the ability to determine its own level of training, which is integral to both robustness and clinical viability. Furthermore, the classifiers should exhibit selectivity, declining to analyse images that fall outside of its capabilities, particularly relevant for lesion types that may not be adequately represented within the training data.

In this chapter, methods for cost-sensitive and selective classification of skin lesions using binary and multi-class deep classification models are investigated. An experimental design that includes multiple disease types represented in the training data and those that are not also utilised. The images utilised in this study were sourced from the ISIC 2019 dataset~\citep{codella2018skin,combalia2019bcn20000,tschandl2018ham10000}.

To add selectivity to a machine learning model, selective classification can be utilised, where the classifier has the option to reject an image if it does not meet certain criteria. The goal of selective classification is to reduce the number of incorrect classifications and increase the overall accuracy of the classifier. In selective classification, a classifier first makes a prediction about the class of an image and then decides whether to accept or reject that prediction based on a criterion such as threshold of confidence. The threshold can be set based on the desired false positive rate or other performance metrics. If the confidence score of the classifier is below the threshold, the image is rejected and not assigned a class label.

Asymmetrical misclassification cost is a technique used in machine learning to address the issue of unequal consequences for different types of classification errors. In medical imaging, false negatives (i.e., misclassifying a malignant skin lesion as benign) can have more serious consequences than false positives (i.e., misclassifying a benign skin lesion as malignant). To address this issue, asymmetrical misclassification cost involves assigning different costs or weights to different types of misclassifications, so that false negatives are penalized more heavily than false positives. This approach allows the classifier to balance sensitivity and specificity in a way that is appropriate for the medical imaging application. By assigning a higher cost to false negatives, the classifier can be encouraged to prioritise sensitivity over specificity.


\subsection{Summary of Work}
\label{subsec:selective_summary_of_work}
The experiments in this study focus on the use of empirical coverage and selective costs to evaluate the performance of selective classification methods in skin lesion analysis. The significance of considering the asymmetry of misclassification costs in both binary (benign vs malignant) and multi-class disease classification scenarios is emphasized. A comprehensive evaluation of various selective classification methods, including predictive probability calibration, uncertainty estimation, and selective classification models, is carried out. A novel selective classification model, Expected Cost SelectiveNet (EC-SelectiveNet) is introduced and analysed in the study. EC-SelectiveNet is based on the SelectiveNet model~\cite{selective2019geifman} and makes selection decisions based on expected costs, rather than on the image rejection rate. This model discards the additional heads used in SelectiveNet (selection and auxiliary heads) and relies solely on the expected costs for image selection during inference.

This work was presented at the Uncertainty for Safe Utilization of Machine Learning in Medical Imaging 2021 (UNSURE) workshop hosted at Medical Image Computing and Computing Assisted Intervention (MICCAI) in Strasbourg, France and published as part of its proceedings~\citep{carse2021robust}.



\section{Literature Review}
\label{sec:selective_review}
Review



\section{Asymmetrical Selective Classification}
\label{sec:selective_classification}
The process of selective classification involves two key components: the selection function and the prediction function. The selection function, denoted as $\sigma(x)$, determines whether or not an image $x$ should be included in the classification process. If an image is rejected, then $\sigma(x)=0$, and if it is selected, then $\sigma(x)=1$. The empirical coverage, $\phi(\sigma|S)$, is defined as the proportion of images selected for classification, calculated as the mean of the selection function over the total number of images in the data set $S$. The prediction function, $P(x)$, is used to make a classification decision for each selected image, and each decision incurs a cost. The average cost over the selected images is referred to as the empirical selective cost.

The mis-classification costs can be specified in a matrix $C$, where $C_{jk}$ is the cost of assigning class $k$ when the true class is $j$. These costs are specific to the deployment setting and are influenced by various factors such as health economics, quality of life considerations, and available treatments. In many reported experiments on dermatology image classification, a symmetric cost matrix is used, i.e., $C = \mathbf{1} - I$, where $\mathbf{1}$ is a matrix of ones and $I$ is the identity matrix. However, this assumption of symmetry is unrealistic, and in many medical classification tasks, the costs are highly asymmetric.

For instance, in the binary classification of malignant (class 1) and benign (class 0) lesions, the cost matrix may reflect that mis-classifying a malignant lesion as benign is much more costly than the reverse mis-classification. In this scenario, $C_{1,0} = 10.0$, $C_{0,1} = 1.0$, $C_{1,1} = 0.0$, and $C_{0,0} = 0.0$. For multiple lesion classes, the cost matrix may be even more complex and should be decided in consultation with relevant stakeholders such as general practitioners, patient representative groups, and health economists. It is important to note that the values used for asymmetric costs should vary depending on the specific clinical setting and should be determined through discussions with relevant experts.

\begin{figure}[!h]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{images/cost_matrix_1.png}
		\caption{}
		\label{fig:multiclasssymcosts}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{images/cost_matrix_2.png}
		\caption{}
		\label{fig:multiclassasymcosts}
	\end{subfigure}
	\caption{Visual representation of the cost matrices for the classes of the ISIC 2019 dataset. Panel (a) depicts a symmetrical cost matrix, in which the costs of misclassification are equivalent. Conversely, panel (b) illustrates an asymmetrical cost matrix, where the costs of misclassification are differentiated based on various factors.}
	\label{fig:multiclasscosts}
\end{figure}

Optimizing classifiers for a particular cost matrix may seem like a viable solution, however, it is not recommended due to the potential changes in the cost matrix after implementation. This would necessitate the frequent retraining of the classifiers, which can become a tedious and time-consuming process.

Given a trained classifier that outputs a calibrated posterior distribution $P(x)$ over classes $T$ for an image $x$, the expected costs of classification can be utilized to make a decision on the image's classification. In the binary classification scenario of benign ($t=0$) and malignant ($t=1$) classes, the expected cost of a benign classification is defined as $R_0 = C_{10} P(t=1|x)$, while the expected cost of a malignant classification is expressed as $R_1 = C_{01} P(t=0|x)$. The image $x$ is classified as malignant if $R_1 < R_0$, otherwise it is classified as benign. In the case of multiple classes, the expected cost for each class is calculated as the sum of costs incurred for each class assuming it to be the true class, and the class with the minimum expected cost is selected as the final decision (Equation \ref{eq:expected_cost_multi}).

\begin{equation}
	\sigma_{EC}(x) = \min_{j} \sum_{t=1}^{T} C_{jt} P(t|x)
	\label{eq:expected_cost_multi}
\end{equation}



\section{Selective Classification Methods}
\label{sec:slective_classification_methods}

\subsection{Predictive Probabilities}
\label{subsec:selective_predictive_probabilites}
The softmax response selection function $\sigma_{SR}(x)$ is computed by determining the maximum value of the prediction function $P(x)$ (Equation \ref{eq:softmax_response}). This assumes that the neural network model utilized has employed a softmax activation function to generate predictive probabilities. While this methodology is straightforward to implement and intuitive, it has limitations as the output probabilities from a softmax activation are not properly calibrated and do not reflect the uncertainty of the model, as demonstrated by~\citep{gal2016dropout}.

\begin{equation}
	\sigma_{SR}(x) = \max_{t} P(T|x)
	\label{eq:softmax_response}
\end{equation}

Temperature scaling is a method utilized for calibrating the output probabilities of a neural network model. This method was chosen due to its demonstrated effectiveness in calibrating predictive probabilities for medical images as well as its ease of implementation. The temperature scaling technique involves scaling the output logits of a neural network with a temperature value $temp > 0$, which is optimized on a validation set during the model's training process. The temperature value is determined from the training epoch that exhibits the lowest validation loss. This technique can be incorporated into a selection function by modifying the prediction function $P(x)$ to include the scaled logits $P_{TS}(x)$ and taking the maximum predicted probability (as shown in Equation \ref{eq:temperature_scaling}).

\begin{equation}
	\sigma_{TS}(x) = \max_t P_{TS}(T|x)
	\label{eq:selective_temperature_scaling}
\end{equation}

\subsection{Bayesian Uncertainty}
\label{subsec:selective_uncertainity}
Bayesian neural networks represent a promising approach for improving the accuracy of probabilistic predictions and for more effectively estimating uncertainty. This is achieved through the representation of model parameters using distributions that are sampled from the forward propagation of the network.

In this study, we evaluate the Bayesian neural network methodology using two methods, namely Bayes by Backprop~\citep{blundell2015weight} and Laplace Approximation~\citep{mackay1992bayesian}, for training the network. The resulting Bayesian neural network can be sampled $M$ times and the average of the predictions (Equation \ref{eq:avg_baysian_neural_network}) can be used to produce more calibrated probabilities, as demonstrated by~\citep{jospin2020hands}. The uncertainty in the Bayesian neural network can be estimated through the variance of the predictive samples (Equation \ref{eq:var_baysian_neural_network}) and can be employed as a method of selection.

\begin{equation}
	\sigma_{AVG}(x) = \max_t \frac{1}{M} \sum_m^M P_{w^m}(T|x)
	\label{eq:avg_baysian_neural_network}
\end{equation}

\begin{equation}
	\sigma_{VAR}(x) = \frac{\sum_m^M (P_{w^m}(T|x) - \mu)^2}{M - 1} \qquad \mu = \frac{1}{M} \sum_m^M P_{w^m}(T|x)
	\label{eq:var_baysian_neural_network}
\end{equation}

Multiple methods exist for estimating the uncertainty of Bayesian neural networks, including variation ratios~\citep{freeman1965elementary}, which measure the spread of the distribution of sample predictions around the mode (Equation \ref{eq:variation_ratio}).

\begin{equation}
	\sigma_{VR}(x) = 1 - \frac{P(T|x)}{M}    
	\label{eq:variation_ratio}
\end{equation}

\noindent Predictive entropy~\citep{shannon1948mathematical}, which captures the average information content of the distribution of sample predictions (Equation \ref{eq:predictive_entropy}).
   
\begin{equation}
	\sigma_{PE}(x) = -\sum^{T}_{t=1}\left(\frac{1}{M}\sum^{M}_{m=1}p_{w^m}(t|x)\right)log\left(\frac{1}{M}\sum^{M}_{m=1}p_{w^m}(t|x)\right)
	\label{eq:predictive_entropy}
\end{equation}

\noindent Mutual information~\citep{houlsby2011bayesian}, which quantifies the relationship between the predictive samples and the posterior distribution over the parameters of the model (Equation \ref{eq:mutual_information}).

\begin{equation}
	\sigma_{MI}(x)=\sigma_{PE}(x)+\frac{1}{T}\sum^{T}_{t=1}\sum^{M}_{m=1}p_{w^m}(t|x)\log p_{w^m}(t|x)
	\label{eq:mutual_information}
\end{equation}

\subsection{SelectiveNet}
\label{subsec:selectivenet}
In the context of neural networks or Bayesian neural networks, data representations optimized for classification have been widely studied. However, \cite{selective2019geifman} posits that data representations can also be optimized for scenarios where a portion of the data is anticipated to be rejected. To address this issue, the author introduces SelectiveNet, a modified training approach for neural networks that enables end-to-end optimization for a specific target coverage.

This is achieved by adding two heads to the model's encoder, in addition to the predictive head (denoted as $P(x)$). These heads consist of a selective head (denoted as $G(x)$) that outputs a selection score and an auxiliary head (denoted as $A(x)$) that provides predictions used within the loss function. The overall loss function used to optimize the entire model is based on selective risk and balances the predictive and selective heads against the auxiliary head to ensure that robust features for classification are learned while still optimizing for target coverage.

\begin{equation}
	L = \alpha L_{p, g} + (1 - \alpha)L_a
	\label{eq:selective_loss}
\end{equation}

The SelectiveNet loss function (Equation~\ref{eq:selective_loss}) is a combination of two functions ($L_{p,g}$ and $L_a$), weighted by a hyperparameter $\alpha$ to control the relative importance of coverage optimization. The first term uses both the predictive and selective heads (Equation~\ref{eq:selectivenet_loss}) and combines cross-entropy loss with coverage. The hyperparameter $t$ represents the target coverage for the model, while $\lambda$ regulates the significance of this target coverage. On the other hand, the auxiliary head uses a standard cross-entropy loss ($L_a$) to encourage the model to learn robust features from the training data. 

\begin{equation}
	L_{p, g} = \frac{1}{N \phi(g)}\sum^{N}_{i=1}l(p(x_i), y_i) g(x_i) + \lambda\max(t - \phi(g|S), 0)^2
	\label{eq:selectivenet_loss}
\end{equation}

\noindent For selective classification, the output of the selective head (Equation~\ref{eq:selectivenet}) is utilized.

\begin{equation}
	\sigma_{SN}(x) = G(x)
	\label{eq:selectivenet}
\end{equation}

\subsection{Expected Cost SelectiveNet}
\label{subsec:ec_selectivenet}
Expected costs serve as a method for selection in both the Convolutional Neural Network (CNN) and the SelectiveNet model, as evidenced by Equation \ref{eq:expected_cost_multi}. In this regard, we propose a new approach to selection, referred to as Expected Cost SelectiveNet (EC-SelectiveNet), which is based on expected costs computed from the predictive head, instead of the selective head output utilized in SelectiveNet.

Despite the fact that SelectiveNet directly outputs a selection score, our proposed EC-SelectiveNet method utilizes the expected costs computed from the predictive head for selection. The selective head is used during training to guide representation learning but, in contrast to the approach presented in \cite{selective2019geifman}, we discard both the selective head and auxiliary head at test time.



\section{Binary Experiments}
\label{sec:selective_binary_experiment}
This section details the datasets, training parameters, experimental setup and results for the experiments with binary asymmetric selective classification for skin lesion triage. The code and full results used within this section can be found on the project GitHub repository~\footnote{GitHub Repository: \url{github.com/UoD-CVIP/Selective_Dermatology}}.

\subsection{Experiment Setup}

\subsection{Dataset}
The ISIC Challenge 2019~\citep{codella2018skin,combalia2019bcn20000,tschandl2018ham10000}, was employed in this study and consists of a total of 25,331 images spanning eight distinct classes, including melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis, benign keratosis, dermatofibroma, vascular lesion, and squamous cell carcinoma. For the purposes of the experiments, two datasets were compiled from the ISIC 2019 data, referred to as $S_{in}$ and $S_{unknown}$.

\begin{figure}[h]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.3\linewidth]{images/MEL.jpg}
		\includegraphics[width=0.3\linewidth]{images/NV.jpg}
		\caption{\(S_{in}\)}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.3\linewidth]{images/SCC.jpg}
		\includegraphics[width=0.3\linewidth]{images/BLK.jpg}
		\caption{\(S_{unknown}\)}
	\end{subfigure}
	\caption{Example images from the test data sets \(S_{in}\) and \(S_{unknown}\).}
	\label{fig:isic_dataset_examples}
\end{figure}

$S_{in}$: These data encompassed the melanoma, melanocytic nevus, and basal cell carcinoma (BCC) images from the ISIC 2019 dataset, which were assigned to two classes for classification: malignant (melanoma, BCC) and benign (melanocytic nevus). The $S_{in}$ dataset was split into three subsets for training, validation, and testing, containing 12432, 3316, and 4972 images, respectively.

$S_{unknown}$: : These data consisted of 4,360 images from classes that were not present in $S_{in}$, including benign keratosis, dermatofibroma, actinic keratosis, and squamous cell carcinoma, and were assigned to either the malignant or benign class. The $S_{unknown}$ dataset was not utilized for training, but instead was employed to test the performance of selective classification on images from disease types not represented in the training data.

The combination of the $S_{in}$ and $S_{unknown}$ test sets is referred to as the $S_{combined}$ dataset. Figure \ref{fig:isic_dataset_examples} provides illustrative examples from the ISIC 2019 dataset.


\subsection{Training Parameters}

\subsection{Results}
\subsubsection{SelectiveNet: Effect of Target Coverage}
We examined the effect of the SelectiveNet target-coverage parameter, $t$, when SelectiveNet's selection head is used to make selection decisions. Figure \ref{fig:sn_coverage} shows cost-coverage curves for values of $t$ ranging from $0.7$ to $1.0$. These are plotted for $S_{in}$, $S_{unknown}$, and $S_{combined}$. 

We expected to find, in accordance with the intended purpose of this parameter, that lower values of $t$ would be effective at lower coverage. On the contrary, training with $t=1.0$ incurred the lowest test cost on $S_{in}$ for coverage values as low as $0.2$. Costs incurred on $S_{unknown}$ are higher as expected, and curves show no clear ordering; the $t=1.0$ curve, however, does show a clear reduction in cost as coverage is reduced.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{images/selectivenet_v2.png}
	\caption{Cost-coverage curves for SelectiveNets trained with different target coverages. From left to right: \(S_{in}\), \(S_{unknown}\) and  \(S_{combined}\).}
	\label{fig:sn_coverage}
\end{figure}

\subsubsection{Does SelectiveNet Training Help?}
The extent to which the target coverage $t$ is enforced is controlled by the weighting parameter $\lambda$. Even when set to target full coverage ($t=1.0$), the model can trade off coverage for cost in extreme cases during training. For this reason, results obtained by SelectiveNet with $t=1.0$ will differ from those obtained by training a network without selective and auxiliary heads. We trained such a network using cross-entropy loss, retaining only the softmax predictive head. It made selection decisions at test time based on the maximum softmax output. The resulting cost-coverage curve is plotted in Figure \ref{fig:sn_coverage} (labelled `softmax'). SelectiveNet trained with a target coverage of 1.0 performed better than a standard CNN with softmax for any coverage above $0.4$.

\subsubsection{MC-Dropout, Temperature Scaling, and EC-SelectiveNet}
We investigated the effect of MC-Dropout on selective classification, using the mean and variance of the Monte Carlo iterations as selection scores, respectively. Figure \ref{fig:mc_dropout} compares the resulting cost-coverage curves with those obtained using a network with no dropout at test time (`softmax response'). On $S_{in}$, using the MC-Dropout average had negligible effect whereas MC variance performed a little worse than simply using the maximum softmax response. In contrast, gains in cost were obtained by MC variance on $S_{unknown}$ for which model uncertainty should be high.
   
\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{images/mc_dropout_v2.png}
	\caption{Cost-coverage curves using MC-Dropout on $S_{in}$, $S_{unknown}$, and $S_{combined}$}
	\label{fig:mc_dropout}
\end{figure}
   
Figure \ref{fig:dataset_splits} plots curves for a softmax network using temperature scaling (trained with cross-entropy loss). Although temperature scaling improved calibration it had negligible effect on cost-coverage curves.  Figure \ref{fig:dataset_splits} also shows curves obtained using EC-SelectiveNet in which the selection head is dropped at test time. EC-SelectiveNet showed a clear benefit on both $S_{in}$ and $S_{unknown}$ compared to training a softmax network without the additional heads.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{images/datasets_v2.png}
	\caption{Cost-coverage curves. From left to right: \(S_{in}\), \(S_{unknown}\) and \(S_{combined}\).}
	\label{fig:dataset_splits}
\end{figure}

\subsubsection{Asymmetric Costs}
We investigated the effect of asymmetric mis-classification costs. Figure \ref{fig:sn_costs} compares SelectiveNet with EC-SelectiveNet ($t=1.0$). They performed similarly when costs were symmetric with SelectiveNet achieving a small cost reduction (approximately $0.015$) at middling coverage. However, in the more realistic asymmetric settings, EC-SelectiveNet achieved cost reductions of approximately $0.1$ at all coverages below about $0.8$.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{images/selectivenet_cost.png}
	\caption{Cost-coverage curves for SelectiveNet and EC-SelectiveNet. From left to right: $C_{1,0}=1$ (symmetric costs), $10$, and $50$ (highly asymmetric costs)}
	\label{fig:sn_costs}
\end{figure}

Figure \ref{fig:temp_scaling} plots the effect of temperature scaling. Both the softmax response and temperature scaling selection methods are based on the expected costs. The effect of temperature scaling was negligible with symmetric costs. In the asymmetric settings it had a small effect on selective classification. This effect was similar whether using EC-SelectiveNet ($t=1.0$) or standard network training with cross-entropy loss. In both cases, temperature scaling increased costs at high coverage and reduced costs at low coverage. Figure \ref{fig:temp_scaling} also makes clear the relative advantage of EC-SelectiveNet.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{images/tmp_scaling_v2.png}
	\caption{Cost-coverage curves for cross-entropy training and EC-SelectiveNet combined with temperature scaling. From left to right: $C_{1,0}=1$ (symmetric costs), $10$, and $50$ (highly asymmetric costs)}
	\label{fig:temp_scaling}
\end{figure}

\section{Multi class Experiments}
\label{sec:selective_multi_class_experiments}
This section details the datasets, training parameters, experimental setup and results for the experiments with binary asymmetric selective classification for skin lesion classification. The code and full results used within this section can be found on the project GitHub repository~\footnote{GitHub Repository: \url{github.com/UoD-CVIP/Asymetric_Selective_Dermatology}}.

\subsection{Dataset}
The ISIC Challenge 2019~\citep{codella2018skin,combalia2019csn20000,tschandl2018ham10000} dataset which consists in total of 25,331 images covering 8 classes: melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis, benign keratosis, dermatofibroma, vascular lesion, and squamous cell carcinoma was used for this study. For this study the dataset set is randomly split into training, validation and testing with the ratio $0.6:0.2:0.2$. Each image is first normalised using the standard deviation and mean across each colour channel. Each of the images in the dataset is squared by cropping the horizontal sides evenly until the image is square, each image is then resized to 256 by 256. Augmentation is used during training by applying random augmentations to each image as it is loaded each epoch. Random 90 degree rotations, horizontal and vertical flips, make up the set of augmentations.

\subsection{Experiment Setup}

\subsection{Training Parameters}

\subsection{Results}



\section{Conclusion}
\label{sec:selective_conclusion}
This study set out to better understand selective classification of skin lesions using asymmetric costs. In a primary care setting, for example, the cost of mis-classifying a life-threatening melanoma is clearly greater than that of misclassifying a benign lesion. We also investigated selective classification with lesion types not adequately represented during training. Generally, EC-SelectiveNet was effective for robust selective classification when trained with a target coverage at (or close to) 1.0. EC-SelectiveNet produced similar or better cost-coverage curves than SelectiveNet.

MC-Dropout averaging made little difference but we note that variance gave encouraging results on \(S_{unknown}\). Temperature scaling to calibrate output probabilities worsened costs at higher coverage. Future work should investigate use of asymmetric cost matrices in multi-class settings, as well as how so-called out-of-distribution detection methods can help in the context of selective skin lesion classification as investigated here.