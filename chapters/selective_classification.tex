\section{Introduction}
\label{sec:selective_introduction}
\subsection{Robust Selective Classification for Skin\\Lesions}
\label{subsec:robust and selective}
The utilisation of automated image analysis for the assessment of skin lesions holds great promise in enhancing diagnostic accuracy and streamlining clinical workflows within the field of dermatology. By employing lesion classifiers that generate class probability distributions, it becomes possible to estimate the associated costs of clinical decisions, such as referral recommendations, thereby facilitating informed decision making.

It is important to note that the costs resulting from misclassification are typically asymmetric, with a greater impact associated with falsely categorising a malignant lesion as benign compared to the reverse. To achieve optimal decision making, it is crucial that the predicted class probabilities are properly calibrated. Additionally, a clinically practical system should possess the ability to determine its own level of training, which is integral to both robustness and clinical viability. Furthermore, the classifiers should exhibit selectivity, declining to analyse images that fall outside of their capabilities, particularly relevant for lesion types that may not be adequately represented within the training data.

In this chapter, methods for cost-sensitive and selective classification of skin lesions using binary and multi-class deep classification models are investigated. An experimental design that includes both binary (malignant vs benign) and multi-class classification tasks. The images utilised in this study were sourced from the ISIC 2019 dataset~\citep{codella2018skin,combalia2019bcn20000,tschandl2018ham10000}.

To add selectivity to a machine learning model, selective classification can be utilised, where the classifier has the option to reject an image if it does not meet certain criteria. The goal of selective classification is to reduce the number of incorrect classifications and decrease the occurred costs of the predictions. In selective classification, a classifier decides whether to accept or reject an input then if accepted makes a prediction on the input. Sometimes the prediction is made first and is used to inform the rejection decision, for example thresholding the prediction confidence. The threshold can be set based on the desired false positive rate or other performance metrics. If the confidence score of the classifier is below the threshold, the image is rejected and not assigned a class label.

Asymmetric misclassification costs are important in the machine learning arena because they provide a framework for dealing with the issues that arise from uneven consequences of classification failures. The repercussions of false negatives and false positives are intrinsically unequal in the context of medical imaging. In the diagnosis of skin lesions, for example, failing to detect a malignant lesion (false negative) has considerably more serious repercussions than incorrectly recognising a benign lesion as malignant (false positive). The former can result in delayed crucial care, potentially jeopardising the patient's health or life, whilst the latter can result in unnecessary stress and follow-up operations, albeit with less severe consequences.

Using Asymmetrical Misclassification Costs entails attributing higher costs to false negatives than false positives; this paradigm acknowledges the increased importance of reducing severe errors. This strategic cost allocation directs the classifier's decision-making process. The increased cost of false negatives motivates the classifier to favour increased sensitivity - the ability to correctly identify true positives. As a result, the classifier is purposefully built to be more careful and meticulous during classification, with a strong emphasis on minimising false negatives. While attempting to improve sensitivity over specificity, the model may demonstrate a tendency to overestimate its predictive powers. This can show as overconfidence in its forecasts, resulting in a distorted probability distribution. The model's proclivity to assign high probability to its predictions may jeopardise calibration, resulting in a misalignment between projected and actual outcomes.

\subsection{Summary of Work}
\label{subsec:selective_summary_of_work}
The experiments in this study focus on the use of empirical coverage and selective costs to evaluate the performance of selective classification methods in skin lesion analysis. The significance of considering the asymmetry of misclassification costs in both binary (benign vs malignant) and multi-class disease classification scenarios is emphasised. An extensive evaluation of various selective classification methods, including predictive probability calibration, uncertainty estimation, and selective classification models, is carried out. A novel selective classification model, Expected Cost SelectiveNet (EC-SelectiveNet), is introduced and analysed. EC-SelectiveNet is based on the SelectiveNet model~\citep{geifman2019selectivenet} and makes selection decisions based on expected costs, rather than on the image rejection rate. EC-SelectiveNet discards the additional heads used in SelectiveNet (selection and auxiliary heads) and relies solely on the expected costs for image selection.

An earlier version of this work was presented at the Uncertainty for Safe Utilization of Machine Learning in Medical Imaging 2021 (UNSURE) workshop hosted at Medical Image Computing and Computing Assisted Intervention (MICCAI) in Strasbourg, France and published as part of its proceedings \citep{carse2021robust}.



\section{Literature Review}
\label{sec:selective_review}
Selective classification was initially introduced by \cite{chow1957optimum}, who explored the concept of a rejection option. Subsequently, this notion was further characterised as a risk-coverage trade-off in the literature~\citep{el2010foundations}. Various authors have endeavoured to construct algorithms that can optimally achieve the best trade-offs. The majority of the research in this area has focused on traditional machine learning methods, such as support vector machines and nearest neighbours~\citep{hellman1970nearest,fumera2002support,wiener2015agnostic}. More recently, \cite{cortes2016learning} proposed a method for jointly learning prediction and selection functions instead of relying on conventional confidence-based rejection. The authors demonstrated that their approach yielded promising outcomes when compared to other selective classification experiments without having to rely on methods that produce noise-free confidence predictions.

\cite{geifman2017selective} were the pioneers of applying selective classification to deep learning algorithms by proposing a rejection mechanism from the model and an automatic threshold selection method to achieve the desired risk. They utilised the reject options of either the softmax response (maximum softmax prediction) or Monte Carlo dropout. Subsequently, \cite{geifman2019selectivenet} introduced SelectiveNet, a deep learning model that can jointly learn the prediction and selection functions, trained for a specific target coverage. The authors asserted that their model's selective classification performance outperformed the methods against which they compared it, namely, softmax response and Monte Carlo dropout.

The utilisation of predictive probability outputs from neural networks for selective classification can pose a challenge due to the weak calibration of these probabilities, which arises from the softmax function. As such, it is imperative to calibrate the predictions prior to their use in selective classification, as delineated in Chapter~\ref{ch:classification_claibration}. The uncertainty inherent in calibrated predictions can serve as a selection criterion by rejecting samples exhibiting high levels of uncertainty. Bayesian neural networks offer a means of quantifying uncertainty in neural networks and can be trained via a range of methods, including Monte Carlo dropout techniques~\citep{gal2016dropout}, backpropagation with weights treated as random variables~\citep{blundell2015weight}, and fitting a Gaussian distribution to the weights for posterior probabilities~\citep{mackay1992bayesian}. By sampling Bayesian neural networks, uncertainty can be measured from the samples using various methods~\citep{gal2016uncertainty}.



\section{Asymmetrical Selective Classification}
\label{sec:selective_classification}
The process of selective classification involves two key components: the selection function and the prediction function. The selection function, denoted as $\sigma(x)$, determines whether or not an image $x$ should be classified. If an image is rejected, then $\sigma(x)=0$, and if it is selected, then $\sigma(x)=1$. The empirical coverage, $\phi(\sigma|S)$, is defined as the proportion of images selected for classification, calculated as the mean of the selection function over the images in the data set $S$. The prediction function, $P(x)$, is used to make a classification decision for each selected image, and each decision incurs a cost. The average cost over the selected images is referred to as the empirical selective cost.

The mis-classification costs can be specified in a matrix $C$, where $C_{jk}$ is the cost of assigning class $k$ when the true class is $j$. These costs are specific to the deployment setting and are influenced by various factors such as health economics, quality of life considerations, and available treatments. In many reported experiments on dermatology image classification, a symmetric cost matrix is used, i.e., $C = \mathbf{1} - I$, where $\mathbf{1}$ is a matrix of ones and $I$ is the identity matrix. However, this assumption of symmetry is unrealistic, and in many medical classification tasks, the costs are highly asymmetric.

For instance, in the binary classification of malignant (class 1) and benign (class 0) lesions, the cost matrix may reflect that mis-classifying a malignant lesion as benign is much more costly than the reverse mis-classification. In this scenario we might have, $C_{1,0} = 10.0$, $C_{0,1} = 1.0$, $C_{1,1} = 0.0$, and $C_{0,0} = 0.0$ for example. For multiple lesion classes, the cost matrix may be more complex and should be decided in consultation with relevant stakeholders such as general practitioners, patient representative groups, and health economists. It is important to note that the values used for asymmetric costs should vary depending on the specific clinical setting and should be determined through discussions with relevant experts.

\begin{figure}[!h]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{images/cost_matrix_1.png}
		\caption{}
		\label{fig:multiclasssymcosts}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{images/cost_matrix_2.png}
		\caption{}
		\label{fig:multiclassasymcosts}
	\end{subfigure}
	\caption{Cost matrices for the classes of the ISIC 2019 dataset. (a) A symmetrical cost matrix, in which the costs of misclassification are equivalent. (b) An asymmetrical cost matrix, where the costs of misclassification are differentiated based on various factors.}
	\label{fig:multiclasscosts}
\end{figure}

Optimising classifiers for a particular cost matrix may seem like a viable solution, however, it is not recommended due to the potential changes in the cost matrix after implementation. This would necessitate the frequent retraining of the classifiers, which can become a tedious and time-consuming process.

Given a trained classifier that outputs a calibrated posterior distribution $P(x, \theta)$ over classes $T$ for an image $x$ using the model parameters $\theta$, the expected costs of classification can be utilised to make a decision on the image's classification~\citep{ferrer2022analysis}. In the binary classification scenario of benign ($t=0$) and malignant ($t=1$) classes, the expected cost of a benign classification is defined as $R_0 = C_{10} P(t=1|x)$, while the expected cost of a malignant classification is expressed as $R_1 = C_{01} P(t=0|x)$. The image $x$ is classified as malignant if $R_1 < R_0$, otherwise it is classified as benign. In the case of multiple classes, the expected cost for each class is calculated as the sum of costs incurred for each class assuming it to be the true class, and the class $\hat{j}$ with the minimum expected cost is selected as the final decision (Equation~(\ref{eq:expected_cost_multi})).

\begin{equation}
	\hat{j} = \argmin_j\sum^T_{t=1}C_{tj}P(t|x,\theta)
	\label{eq:expected_cost_multi}
\end{equation}



\section{Selective Classification Methods}
\label{sec:slective_classification_methods}

\subsection{Predictive Probabilities}
\label{subsec:selective_predictive_probabilites}
The softmax response selection function $\sigma_{SR}(x)$ is computed by determining the maximum value of the prediction function $P(x, \theta)$ with a symmetrical cost matrix. This assumes that the neural network model utilised has employed a softmax activation function to generate predictive probabilities. While this methodology is straightforward to implement and intuitive, it has limitations as the output probabilities from a softmax activation are not properly calibrated and do not reflect the uncertainty of the model, as demonstrated by~\cite{gal2016dropout}.

\begin{equation}
	\sigma_{SR}(x) = \max_tP(t|x,\theta)
	\label{eq:softmax_response}
\end{equation}

\noindent In scenarios where costs are asymmetrical, the expected costs of a classification decision, as denoted by Equation~(\ref{eq:expected_cost_multi}), can be utilised to formulate a selective classification decision function $\sigma_{EC}(x)$ through the employment of the cost matrix $C$. 

\begin{equation}
	\sigma_{EC}(x) = \min_j\sum^T_{t=1}C_{tj}P(t|x,\theta)
	\label{eq:expected_cost_scoring}
\end{equation}

Temperature scaling is a method utilised for calibrating the output probabilities of a neural network model (as discussed in Section~\ref{subsec:post_hoc_calibration}). This method was chosen due to its demonstrated effectiveness in calibrating predictive probabilities for medical images as well as its ease of implementation. The temperature scaling technique involves scaling the output logits of a neural network with a temperature value, which is optimised on a validation set during the model's training process. The temperature value is determined from the training epoch that exhibits the lowest validation loss. This technique can be incorporated into a selection function by modifying the prediction function $P(x, \theta)$ to include dividing the logits by a temperature value before applying a softmax activation (Equation~(\ref{eq:temperature_scaling})) resulting in $P_{TS}(x)$. Equation~(\ref{eq:selective_temperature_scaling}) shows how temperature scaled probabilities can be used for selective classification.

\begin{equation}
	\sigma_{TS}(x) = \max_tP_{TS}(t|x,\theta)
	\label{eq:selective_temperature_scaling}
\end{equation}

\subsection{Bayesian Uncertainty}
\label{subsec:selective_uncertainity}
Bayesian neural networks represent a promising approach for improving the accuracy of probabilistic predictions and for more effectively estimating uncertainty. This is achieved through the representation of model parameters using distributions that can then be sampled from using forward propagation of the network.

In this study, Bayesian neural networks are evaluated using two methods, namely Bayes-by-Backprop~\citep{blundell2015weight} and Laplace Approximation~\citep{mackay1992bayesian}, for training the network. The resulting Bayesian neural network can be sampled $M$ times and the average of the predictions (Equation~(\ref{eq:avg_baysian_neural_network})) can be used to produce more calibrated probabilities, as demonstrated by~\citep{jospin2022hands}. The uncertainty in the Bayesian neural network can be estimated through the variance of the predictive samples (Equation~(\ref{eq:var_baysian_neural_network})) and can be employed as a method of selection.

\begin{equation}
	\sigma_{AVG}(x)=\max_t\frac{1}{M}\sum^M_{m=1}P(t|x,\theta_m)
	\label{eq:avg_baysian_neural_network}
\end{equation}

\begin{equation}
	\begin{aligned}
		\sigma_{VAR}(x)&=\frac{\sum^M_{m=1}(P(t|x,\theta_m)-\mu)^2}{M-1}\\
		\mu&=\frac{1}{M}\sum^M_{m=1}P(t|x,\theta_m)
	\end{aligned}
	\label{eq:var_baysian_neural_network}
\end{equation}

Multiple methods exist for estimating the predictive uncertainty of Bayesian neural networks, including variation ratios~\citep{freeman1965elementary}, which measure the spread of the distribution of sample predictions around the mode (Equation~(\ref{eq:variation_ratio})).

\begin{equation}
	\begin{aligned}
		\sigma_{VR}(x) &= 1 - \frac{\sum^M_{m=1}1\!\!1(\argmax_tP(t|x,\theta_m)=\hat{t})}{M}\\
		\qquad\hat{t} &= \argmax_t\sum^M_{m=1}1\!\!1(\argmax_cP(c|x,\theta_m)=t)
	\end{aligned}
	\label{eq:variation_ratio}
\end{equation}

\noindent Predictive entropy~\citep{shannon1948mathematical} captures the average information content of the distribution of sample predictions (Equation~(\ref{eq:predictive_entropy})).
   
\begin{equation}
	\sigma_{PE}(X)=-\sum^T_t=1\left(\frac{1}{M}\sum^M_{m=1}P(t|x,\theta_m)\right)\log\left(\frac{1}{M}\sum^M_{m=1}P(t|x,\theta_m)\right)
	\label{eq:predictive_entropy}
\end{equation}

\noindent Mutual information~\citep{houlsby2011bayesian} quantifies the relationship between the predictive samples and the posterior distribution over the parameters of the model (Equation~(\ref{eq:mutual_information})).

\begin{equation}
	\sigma_{MI}(x) = \sigma_{PE} + \frac{1}{M}\sum^M_{m=1}\sum^T_{t=1}P(t|x,\theta_m)\log P(t|x,\theta_m)
	\label{eq:mutual_information}
\end{equation}

\subsection{SelectiveNet}
\label{subsec:selectivenet}
In the context of neural networks or Bayesian neural networks, data representations optimised for classification have been widely studied. However, \cite{geifman2019selectivenet} posit that data representations can also be optimised for scenarios where a portion of the data is anticipated to be rejected. To address this issue, they introduce SelectiveNet, a modified training approach for neural networks that enables end-to-end optimisation for a specific target coverage (the probability mass of the non-rejected images).

This is achieved by adding two heads to the model's encoder, in addition to the predictive head (denoted as $P(x)$). These heads consist of a selective head (denoted as $G(x)$) that outputs a selection score and an auxiliary head (denoted as $A(x)$) that provides predictions used within the loss function. The overall loss function used to optimise the entire model is based on selective risk and balances the predictive and selective heads against the auxiliary head to ensure that robust features for classification are learned while still optimising for target coverage. The SelectiveNet loss function (Equation~(\ref{eq:selective_loss})) is a combination of two functions ($L_{p,g}$ and $L_a$), weighted by a hyperparameter $\alpha$ to control the relative importance of coverage optimisation.

\begin{equation}
	L = \alpha L_{p, g} + (1 - \alpha)L_a
	\label{eq:selective_loss}
\end{equation}

The first term uses both the predictive and selective heads (Equation~(\ref{eq:selectivenet_loss})) and combines cross-entropy loss $l$ with coverage $\phi$ (Equation~(\ref{eq:coverage})). For selective classification, the output of the selective head (Equation~(\ref{eq:selectivenet})) is utilised. The hyperparameter $k$ represents the target coverage for the model, while $\lambda$ regulates the significance of this target coverage. On the other hand, the auxiliary head uses a standard cross-entropy loss ($L_a$) to encourage the model to learn robust features from the training data.

\begin{equation}
	L_{p,g}=\frac{\sum^N_{n=1}l(p(x^n,\theta),y)g(x^n,\theta)}{\phi}+\lambda\cdot\max(0,k-\phi)^2
	\label{eq:selectivenet_loss}
\end{equation}

\begin{equation}
	\phi = \frac{1}{N}\sum^N_{n=1}g(x^n,\theta)
	\label{eq:coverage}
\end{equation}

\noindent 

\begin{equation}
	\sigma_{SN}(x) = G(x)
	\label{eq:selectivenet}
\end{equation}

\subsection{Expected Cost SelectiveNet}
\label{subsec:ec_selectivenet}
Expected costs serve as a method for selection in both the CNN and the SelectiveNet model, as evidenced by Equation~(\ref{eq:expected_cost_multi}). A new approach to selection is proposed, referred to as Expected Cost SelectiveNet, which is based on expected costs computed from the predictive head, instead of the selective head output utilised in SelectiveNet.

Despite the fact that SelectiveNet directly outputs a selection score, the proposed EC-SelectiveNet method utilises the expected costs computed from the predictive head for selection. The selective head is used during training to guide representation learning but, in contrast to the approach presented in \cite{geifman2019selectivenet}, both the selective head and auxiliary head are discarded at test time.



\section{Binary Classification Experiments}
\label{sec:selective_binary_experiment}
This section details the datasets, training parameters, experimental setup and results for the experiments with binary asymmetric selective classification for skin lesion triage. The code and full results used within this section can be found on the project GitHub repository~\footnote{GitHub Repository: \url{github.com/UoD-CVIP/Selective_Dermatology}}.

\subsection{Dataset Processing}
\label{subsec:selective_binary_dataset}
The ISIC Challenge 2019~\citep{codella2018skin,combalia2019bcn20000,tschandl2018ham10000} was employed in this study and consists of a total of 25,331 images spanning eight distinct classes, including melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis, benign keratosis, dermatofibroma, vascular lesion, and squamous cell carcinoma. For the purposes of the experiments, two datasets were compiled from the ISIC 2019 data, referred to as $S_{in}$ and $S_{unknown}$.

\begin{figure}[h]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.3\linewidth]{images/MEL.jpg}
		\includegraphics[width=0.3\linewidth]{images/NV.jpg}
		\caption{\(S_{in}\)}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.3\linewidth]{images/SCC.jpg}
		\includegraphics[width=0.3\linewidth]{images/BLK.jpg}
		\caption{\(S_{unknown}\)}
	\end{subfigure}
	\caption{Example images from the test data sets \(S_{in}\) and \(S_{unknown}\).}
	\label{fig:isic_dataset_examples}
\end{figure}

$S_{in}$: These data encompassed the melanoma, melanocytic nevus, and basal cell carcinoma (BCC) images from the ISIC 2019 dataset, which were assigned to two classes for classification: malignant (melanoma, BCC) and benign (melanocytic nevus). The $S_{in}$ dataset was split into three subsets for training, validation, and testing, containing 12432, 3316, and 4972 images, respectively.

$S_{unknown}$: These data consisted of 4,360 images from classes that were not present in $S_{in}$, including benign keratosis, dermatofibroma, actinic keratosis, and squamous cell carcinoma, and were assigned to either the malignant or benign class. The $S_{unknown}$ dataset was not utilised for training, but instead was employed to test the performance of selective classification on images from disease types not represented in the training data.

The combination of the $S_{in}$ and $S_{unknown}$ test sets is referred to as the $S_{combined}$ dataset. Figure \ref{fig:isic_dataset_examples} provides illustrative examples from the ISIC 2019 dataset. In the current study, a random split strategy was employed to divide the dataset into three distinct sets: training, validation, and testing. To ensure comparability across all images, normalisation was performed utilising the standard deviation and mean calculation for each colour channel of the images. Subsequently, each image underwent resizing to 256x256, and during the training phase, data augmentation was carried out by implementing randomised horizontal and vertical flips and rotations of multiples of 90\textdegree.

\subsection{Experiment Setup}
Eight models were trained on the training split of $S_{in}$, utilising the validation set to identify the optimal model from the training epochs. The performance of each selection method was then evaluated by using selection methods with the appropriate model. A detailed overview of the experimental setup is presented in Table~\ref{tab:binary-experiment-setup}. The evaluation of the selection methods on the trained models was carried out utilising a symmetrical cost matrix that is commonly used in such evaluations, where the cost of false positives and false negatives was set to $1.0$. To investigate the effect of adjusting the level of asymmetry, the cost of false positives was set to $10.0$ and $50.0$ while keeping the cost of false negatives fixed at $1.0$. The evaluation of the selection methods was conducted on three different datasets: $S_{in}$ was used to evaluate the in-distribution performance, $S_{unknown}$ was used to assess the generalisation performance on unknown types of skin lesions, and $S_{combines}$ used to evaluate the joint performance on a test set containing in-distribution and unknown types of skin lesions.

\begin{table}[h]
	\centering
	\caption{Binary experiments; trained models and selection methods to be evaluated.}
	\label{tab:binary-experiment-setup}
	\begin{tabular}{|l|l|}
		\hline
		Model & Selection Method \\ \hline
		CNN & Softmax Response \\
		& Temperature Scaled Softmax Response \\ \hline
		SelectiveNet $k=0.7$ & SelectiveNet \\
		SelectiveNet $k=0.75$ & EC-SelectiveNet \\
		SelectiveNet $k=0.8$ & Temperature Scaled EC-SelectiveNet \\
		SelectiveNet $k=0.85$ &  \\
		SelectiveNet $k=0.9$ &  \\
		SelectiveNet $k=0.95$ &  \\
		SelectiveNet $k=1.0$ &  \\ \hline
		Monte Carlo Dropout & Average Softmax Response \\
		& Average Prediction Variance \\ \hline
	\end{tabular}
\end{table}


\subsection{Training Parameters and Model Architecture}
\label{subsec:selective_binary_training}
The implementation of the conventional convolutional neural network comprises an EfficientNet~\citep{tan2019efficientnet} encoder with a compound coefficient of 7. This encoder is followed by an average pooling operation that reduces the width and height by a factor of 8, thereby compressing the encoding size from 163,840 to 2560. Subsequently, the architecture includes a hidden layer equipped with 512 neurons and a rectified linear unit activation function, which is followed by a final output layer with 2 output neurons and a softmax activation function. To mitigate overfitting, dropout regularisation is applied with a drop chance of 0.5 before and after the hidden layer. In total, the architecture encompasses 275 layers (convolutional and fully connected) with 65,099,224 total parameters. Additionally, the use of dropout during training enables the simulation of a Bayesian neural network through Monte Carlo dropout~\citep{gal2016dropout} by sampling the trained model with the same dropout rates.

The SelectiveNet model architecture~\citep{geifman2019selectivenet}, is constructed upon an EfficientNet encoder, followed by average pooling and a fully connected hidden layer containing 512 neurons. The prediction head of the model comprises a single fully connected output layer with 2 neurons and a softmax activation function. The selection head is composed of an additional hidden layer with 512 neurons and a softmax activation function, which is followed by an output layer with a single neuron and a sigmoid activation function. The auxiliary head is similarly structured to the classification head. These additional components result in an increased number of layers in the SelectiveNet architecture, bringing the total to 277 layers, with 65,364,449 parameters in total.

The training configurations were standardised across all experiments. The models were trained utilising 16-bit precision to compute gradients, and Stochastic Gradient Descent was employed to optimise the model parameters. The optimisation process employed a triangular cyclical scheduler~\citep{smith2017cyclical} that cyclically adjusted the learning rate between 0.00001 and 0.1 and the momentum between 0.8 and 0.9 every 2000 training steps. The experiments were performed using mini-batches of 8 images each.

During the training of the conventional convolutional neural network, the cross-entropy loss function was utilised to evaluate the model's performance. To mitigate overfitting, dropout regularisation was applied with a drop rate of 0.5 before both the hidden and output layers. By consistently applying the same dropout pattern during both training and evaluation, the convolutional neural network can be treated as a Bayesian Neural Network and sampled multiple times through Monte Carlo dropout~\citep{gal2016dropout}.

The SelectiveNet model's performance is evaluated using a loss function that integrates the outputs of the three heads, predictive ($p$), selective ($g$), and auxiliary ($h$). The predictive and selective heads are utilised to calculate a portion of the loss $L_{p,g}$, which optimises the model for a specific target coverage. The auxiliary head, in contrast, calculates cross-entropy loss $L_h$, and the two components are weighted together using the parameter $\alpha$ (as specified in Equation \ref{eq:selective_loss}). In the present set of experiments, the parameter $\alpha$ has been set to 0.5 and multiple target coverages have been explored. The remaining hyperparameters associated with the SelectiveNet loss function have been set to the values recommended by the authors in~\citep{geifman2019selectivenet}.

\subsection{Results}
The findings presented in this section are not exhaustive, and complete cost coverage curves for all models and methods of selective classification are available in Appendix~\ref{app:selective_classification_results}.

\subsubsection{SelectiveNet: Effect of Target Coverage}
In order to examine the impact of the SelectiveNet target-coverage parameter, $t$, on the selection decisions made by the SelectiveNet selection head, the cost-coverage curves were plotted for various values of $t$, ranging from $0.7$ to $1.0$ with increments of $0.05$, as depicted in Figure~\ref{fig:binary-selectivenet}. The curves were computed for $S_{in}$, $S_{unknown}$, and $S_{combined}$. According to the design objectives of the target coverage parameter, a lower value of $t$ would result in a more effective model at lower coverages. The findings, however, show that training with a $t=1.0$ value resulted in the lowest test cost on $S_in$ for coverage values as low as $0.2$. As expected, the expenses incurred on $S_unknown$ were larger, and the curves did not show a clear ordering. Nonetheless, the model trained with $t=1.0$ revealed a significant cost reduction as coverage declined. This could be because when the model is trained with a $t=1.0$, the resulting model is closer to the best performing standard model with softmax response.

\begin{figure}[!h]
	\centering
	\includegraphics[width=\textwidth]{images/binary_selectivenet.png}
	\caption{Cost-coverage curves for SelectiveNets trained with different target coverages. From top to bottom: $S_{in}$, $S_{unknown}$ and $S_{combined}$.}
	\label{fig:binary-selectivenet}
\end{figure}

\subsubsection{Does SelectiveNet Training Help?}
The extent to which the target coverage $t$ is imposed is regulated by the weighting parameter $\lambda$. Despite being set to target full coverage ($t=1.0$), the model may, in exceptional circumstances during training, compromise coverage for cost. As a result, the results obtained by SelectiveNet with $t=1.0$ may differ from those obtained through training a network without selective and auxiliary heads. These networks were trained using cross-entropy loss and only retained the softmax predictive head, making selection decisions at test time based on the maximum softmax output. The corresponding cost-coverage curve is plotted in Figure~\ref{fig:binary-selectivenet} (labelled “softmax”). The results indicate that SelectiveNet trained with a target coverage of 1.0 performed better than a standard convolutional neural network with a softmax response for any coverage value above $0.4$.

% TODO - Explain which exceptional circumstances the model may compromise coverage for cost

\begin{figure}[!h]
	\centering
	\includegraphics[width=\textwidth]{images/binary_mcdropout.png}
	\caption{Cost-coverage curves using MC-Dropout. From top to bottom: $S_{in}$, $S_{unknown}$ and $S_{combined}$.}
	\label{fig:binary-mcdropout}
\end{figure}

\subsubsection{MC-Dropout, Temperature Scaling, and EC-SelectiveNet}
The impact of MC-Dropout on selective classification was analysed by using the mean and variance of the Monte Carlo iterations as selection scores. Figure~\ref{fig:binary-mcdropout} compares the resulting cost-coverage curves to those obtained using a network without dropout at test time (labelled “softmax response”). The results reveal that for the $S_{in}$ data, utilising the average of Monte Carlo samples had minimal impact, whereas the variance of the Monte Carlo samples performed slightly worse than simply relying on the maximum softmax response. Conversely, significant cost savings were achieved by using the variance of the Monte Carlo samples on the $S_{unknown}$ data, where model uncertainty is expected to be high.

\begin{figure}[!h]
	\centering
	\includegraphics[width=\textwidth]{images/binary_datasets.png}
	\caption{Cost-coverage curves for different selective classification methods. From top to bottom: $S_{in}$, $S_{unknown}$ and $S_{combined}$.}
	\label{fig:binary-datasets}
\end{figure}


% TODO - Why did temperature scaling have a minimal effect?

The effect of temperature scaling on a softmax network was analysed and the results are shown in Figure~\ref{fig:binary-datasets}. The softmax network was trained using cross-entropy loss and temperature scaling was applied to improve calibration. However, the results indicated that temperature scaling had a minimal effect on the cost-coverage curves. Furthermore, Figure~\ref{fig:binary-datasets} displays the results obtained using EC-SelectiveNet, in which the selection head was omitted during testing. The results demonstrate that EC-SelectiveNet achieved a noticeable improvement on both the $S_{in}$ and $S_{unknown}$ datasets when compared to training a standard convolutional neural network model without the auxiliary heads.

\begin{figure}[!h]
	\centering
	\includegraphics[width=\textwidth]{images/binary_ecselectivenet.png}
	\caption{Cost-coverage curves for SelectiveNet and EC-SelectiveNet. From top to bottom: $C_{1,0}=1$, $C_{1,0}=10$ and $C_{1,0}=50$.}
	\label{fig:binary-ecselectivenet}
\end{figure}

\subsubsection{Asymmetric Costs}
The results of comparing SelectiveNet with EC-SelectiveNet with a target coverage of $t=1.0$ are depicted in Figure~\ref{fig:binary-ecselectivenet}. In symmetric cost scenarios, the performance of both methods was comparable, with SelectiveNet exhibiting a slight advantage in terms of cost, yielding a reduction of approximately $0.015$ at intermediate coverage levels. However, when the cost matrix was asymmetric, EC-SelectiveNet demonstrated significant cost reductions of approximately $0.1$ at all coverages below approximately $0.8$.

\begin{figure}[!h]
	\centering
	\includegraphics[width=\textwidth]{images/binary_asymetrical.png}
	\caption{Cost-coverage curves for cross-entropy training and EC-SelectiveNet combined with temperature scaling. From top to bottom: $C_{1,0}=1$, $C_{1,0}=10$ and $C_{1,0}=50$.}
	\label{fig:binary-asymetric}
\end{figure}

The effect of temperature scaling on selective classification is presented in Figure~\ref{fig:binary-asymetric}. The methodologies of both the softmax response and temperature scaling selection are founded on the principle of expected costs. The impact of temperature scaling was found to be minimal in the context of symmetrical costs. In the asymmetrical cost matrix scenario, a slight effect on selective classification was observed. This effect was consistent regardless of whether EC-SelectiveNet ($t=1.0$) or a convolutional neural network trained with cross-entropy loss was employed. As depicted in Figure~\ref{fig:binary-asymetric}, the application of temperature scaling resulted in an increase in costs at high coverage levels and a reduction in costs at low coverage levels. The figure also highlights the superiority of EC-SelectiveNet in comparison to temperature scaling.



\section{Multi class Experiments}
\label{sec:selective_multi_class_experiments}
This section details the datasets, training parameters, experimental setup and results for the experiments with multi-class asymmetric selective classification for skin lesion classification. The code and full results used within this section can be found on the project GitHub repository~\footnote{GitHub Repository: \url{github.com/UoD-CVIP/Asymetric_Selective_Dermatology}}.

\subsection{Dataset Processing}
In this study, the ISIC Challenge 2019 dataset~\citep{codella2018skin,combalia2019bcn20000,tschandl2018ham10000} was employed. The data was processed using a similar methodology to the one described in Section~\ref{subsec:selective_binary_dataset}. A random splitting method was utilised to divide the dataset into three subsets: training, validation, and testing, in a 60:20:20 ratio. Prior to the training process, the images underwent normalisation, which involved computing the standard deviation and mean across each colour channel. Then, the images were square cropped by evenly trimming the horizontal sides and resized to 256x256. During the training phase, data augmentation was applied by randomly augmenting the images at each epoch. The augmentations consisted of 90-degree rotations and horizontal and vertical flips.

\subsection{Experiment Setup}
In the multi-class set of experiments, all models and methods were subjected to repetition thrice, with the final results being an average of the outcomes from each run. In order to ensure a comprehensive evaluation, the training, validation, and testing splits were randomised for each of the three repetitions. A total of eleven models were trained, utilising the validation set to determine the most suitable model. Table~\ref{tab:multi-class-experiment-setup} shows the different trained models and the selection methods evaluated on each model.

\begin{table}[h]
	\centering
	\caption{Multi-class experiments; trained models and selection methods to be evaluated.}
	\label{tab:multi-class-experiment-setup}
	\begin{tabular}{|l|l|}
		\hline
		Model & Selection Method \\ \hline
		CNN & Softmax Response \\
		& Expected Costs \\
		& Temperature Scaled Softmax Response \\
		& Temperature Scaled Expected Costs \\ \hline
		SelectiveNet $k=0.7$ & SelectiveNet \\
		SelectiveNet $k=0.75$ & Softmax Response \\
		SelectiveNet $k=0.8$ & Temperature Scaled Softmax Response \\
		SelectiveNet $k=0.85$ & EC-SelectiveNet \\
		SelectiveNet $k=0.9$ & Temperature Scaled Expected Costs \\
		SelectiveNet $k=0.95$ &  \\
		SelectiveNet $k=1.0$ &  \\ \hline
		Monte Carlo Dropout & Average Softmax Response \\
		Bayes By Backprop & Average Expected Costs \\
		Laplace Approximation & Bayesian Sample Agreement \\
		& Average Variance \\
		& Predicted Class Variance \\
		& Predictive Entropy \\
		& Variational Ratio \\
		& Mutual Information \\ \hline
	\end{tabular}
\end{table}

The efficacy of the selection methods employed with each of the models was evaluated using both a typical symmetrical cost matrix (Figure~\ref{fig:multiclasssymcosts}) and an asymmetrical cost matrix (Figure~\ref{fig:multiclassasymcosts}). The asymmetrical cost matrix was developed by a consultant-level dermatologist, taking into account the clinical costs associated with misdiagnosis such as death or unnecessary treatment. The values assigned to the asymmetric costs are context-dependent and should be arrived at through engagement with relevant experts, taking into consideration the specific clinical setting.

\subsection{Training Parameters and Model Architecture}
The architecture of the CNN and SelectiveNet in this study was consistent with the binary experiments outlined in Section~\ref{subsec:selective_binary_training}. The number of output neurons in the output layer of the CNN and the predictive and auxiliary heads of the SelectiveNet architecture, was increased to 8, resulting in a total of 65,102,296 weights for the CNN model and 65,370,593 weights for the SelectiveNet model.

Additionally, the Bayes-by-Backprop model~\citep{blundell2015weight} utilised in this study was implemented as a CNN, with the final two fully connected layers replaced by fully connected Bayesian layers. In these layers, each weight is represented by a normal distribution with a mean randomly selected from a range of 0 to 0.1 and a standard deviation sampled from a range of -7 to 0.1. The weight priors $P(\theta)$ was modelled using a scale mixture of two Gaussian distributions (Equation~(\ref{eq:scalemixture})) where $J$ represented the number of weights in a model with standard deviation values ($\sigma_1$ and $\sigma_2$) of 0.1 and 0.4 and a $\pi$ weighting the two Gaussian distributions of 0.5. 

\begin{equation}
	P(\theta) = \prod^J_{j=1}\pi M(\theta_j|0,\sigma_1)+(1-\pi)M(\theta_j|0,\sigma_2)
	\label{eq:scalemixture}
\end{equation}

The CNN and SelectiveNet models were trained with the same weights as those presented in the binary experiments described in Section~\ref{subsec:selective_binary_training}. The Bayesian Neural Network model, which was trained using the Bayes-by-Backprop method~\citep{blundell2015weight}, employed variational inference to approximate the posterior distribution over weights, $q(\theta)$. The weights for the distribution, $\theta$, were determined by minimising the KL-Divergence between the variational posterior and the true posterior. The true posterior was estimated through Monte Carlo sampling of the evidence lower bound, as expressed in Equation~(\ref{eq:elbo}), where $D$ represents the dataset and $M$ denotes the number of Monte Carlo samples.

The loss function utilised for training the Bayesian Neural Network model was a combination of the ELBO and cross-entropy. The weight assigned to the ELBO component of the loss function was modulated based on the current mini-batch, as depicted in Equation \ref{eq:elbo_weighting}, where $N$ represents the number of mini-batches per epoch and $n$ denotes the current mini-batch. This weighting approach was employed such that the early mini-batches were more influenced by Bayesian complexity and later mini-batches focused more on learning from the training data. In the experiments, the Bayesian neural network model trained with the Bayes by Backprop method was trained using a weighted combination of ELBO and cross-entropy, with the ELBO estimated using three Monte Carlo samples during training. All other training settings were equivalent to those utilised for the training of the convolutional neural network model.

\begin{equation}
	\pi_n = \frac{2^{N-n}}{2^N-1}
	\label{eq:elbo_weighting}
\end{equation}

After training, a CNN model can be transformed into a Bayesian neural network using the Laplace approximation method~\citep{mackay1992bayesian}. The Laplace approximation is a technique for approximating the posterior of a model as a Gaussian distribution centred on the learned weights. The curvature of the approximation is estimated through the use of approximations to the Hessian matrix~\citep{botev2017practical} at the MAP. In the experiments, Laplace approximation was applied only to the last layer of the neural network due to hardware constraints, however, it has been demonstrated by~\cite{kristiadi2020being} that this approach can lead to improved calibration and estimation of predictive uncertainty. After performing the Laplace approximation, a predictive probability can be computed by averaging Monte Carlo samples.


\subsection{Results}
The findings presented in this section are not exhaustive, and complete cost coverage curves for all models and methods of selective classification are available in Appendix~\ref{app:selective_classification_results}.

% TODO - Explain the performance reduction with the selective classification with multi-class asymmetrical costs. Maybe quantify this.

\subsubsection{SelectiveNet, EC-SelectiveNet and Target Coverage}
Figure~\ref{fig:multi-class-selective} illustrates the outcomes of the investigation carried out on SelectiveNet, which was trained with different target coverages ranging from $0.7$ to $1.0$. The utilisation of the selective head of the SelectiveNet model resulted in suboptimal performance for selective classification in both settings. In contrast, using the predictive head in symmetrical settings led to improved performance compared to the selective head, resulting in a similar curve shape as that of the CNN. However, it is worth noting that the base performance, and consequently, the area under the cost coverage curve, was worse in all cases except for SelectiveNet trained with a target coverage of $1.0$. This observation is consistent with the results of the binary classification experiments and can be ascribed to the SelectiveNet model's higher number of weights, which enables it to learn a superior model when trained with a target coverage of $1.0$, as its learning is not constrained. In an asymmetrical cost scenario, the use of predictive heads to select based on estimated cost (EC-SelectiveNet) yielded similar results to those of the binary experiments, where the cost coverage curves trended upwards until they approached their target coverage and then demonstrated better performance.

\begin{figure}[!h]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/multi-class-selective-sym.png}
		\caption{Symmetrical Costs}	
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/multi-class-selective-asym.png}
		\caption{Asymmetrical Costs}
	\end{subfigure}
	\caption{Results with experiments with SelectiveNet.}
	\label{fig:multi-class-selective}
\end{figure}

\subsubsection{Effect of Temperature Scaling}
The outcomes of the experiments involving the convolutional neural network and EC-SelectiveNet (trained with a target coverage of $1.0$) in both symmetrical and asymmetrical cost scenarios are illustrated in Figure~\ref{fig:multi-class-temp}. The choice of the EC-SelectiveNet model with a target coverage of $1.0$ was predicated on its superior performance relative to other SelectiveNet models, as evidenced in Figure~\ref{fig:multi-class-selective}. The findings demonstrate that the utilisation of temperature scaling to improve prediction calibration had a negligible impact on the cost coverage curves in symmetrical cost settings. However, in multi-class situations characterised by asymmetrical costs, temperature scaling had no effect on the convolutional neural network but exhibited a detrimental effect when used in conjunction with the EC-SelectiveNet model.

% TODO - Explain the negative effect.

\begin{figure}[!h]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/multi-class-temp-sym.png}
		\caption{Symmetrical Costs}	
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/multi-class-temp-asym.png}
		\caption{Asymmetrical Costs}
	\end{subfigure}
	\caption{Results with experiments with temperature scaling.}
	\label{fig:multi-class-temp}
\end{figure}

\subsubsection{Methods Bayesian Neural Networks}
In the multi-class experiments, three distinct implementations of Bayesian neural networks were examined. For each Bayesian neural network, 100 samples were taken, and the selection was based on the average of the Monte Carlo samples. The variance of the samples is presented in Figure~\ref{fig:multi-class-bayesian}. In a symmetrical cost scenario, both the sample average and variance of Monte Carlo dropout and Laplace approximation performed similarly, with the average slightly outperforming the variance. However, the model trained using Bayes by Backprop exhibited very poor performance when using the sample variance, in comparison to the average. In the asymmetrical cost scenario, the variance was found to be a significantly better method for selective classification than the expected costs of the sample average. The disparity in performance for the Monte Carlo dropout and Laplace approximation methods was evident in the initial 100\% to 80\% coverage before the curves converge. This was not the case with Bayes by Backprop, as the performance with sample variance remained considerably worse than expected costs using sample average.

\begin{figure}[!h]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/multi-class-bayesian-sym.png}
		\caption{Symmetrical Costs}	
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/multi-class-bayesian-asym.png}
		\caption{Asymmetrical Costs}
	\end{subfigure}
	\caption{Results with experiments with Bayesian Neural Networks.}
	\label{fig:multi-class-bayesian}
\end{figure}

\subsubsection{Measures of Uncertainty}
In the multi-class experiments, various measures of uncertainty were employed with Bayesian neural networks' Monte Carlo samples. The results presented in Figure~\ref{fig:multi-class-uncertainity} depict the outcomes of selective classification with Laplace Approximation models, as they demonstrated the best performance, as shown in Figure~\ref{fig:multi-class-bayesian}. The results indicate that the two measures of variance (average over all classes and top class only) perform comparably, with average variance having a slight edge. Predictive entropy emerged as the best-performing measure of uncertainty for selective classification in both symmetrical and asymmetrical cost settings. While mutual information exhibited inferior performance when compared to variance and predictive entropy, it performed significantly worse in an asymmetrical setting. The variational ratio was the poorest measure, it displayed similar performance to other metrics until 90\% coverage where the performance flattened meaning that the selections to reject are random.

\begin{figure}[!h]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/multi-class-uncertainity-sym.png}
		\caption{Symmetrical Costs}	
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/multi-class-uncertainity-asym.png}
		\caption{Asymmetrical Costs}
	\end{subfigure}
	\caption{Results with experiments with measures of uncertainty.}
	\label{fig:multi-class-uncertainity}
\end{figure}



\section{Conclusion}
\label{sec:selective_conclusion}
The aim of this chapter was to enhance comprehension of the performance of various selective classification methods for skin lesion images, utilising asymmetrical costs in both binary (triage setting) and multi-class (disease) classification contexts. Additionally, the effectiveness of selective classification techniques when dealing with lesion types not present in the training data was investigated. The experimental results indicate that SelectiveNet, in general, was less effective compared to other selective classification methods, and only exhibited improvement in performance when trained with a target coverage of $1.0$. Furthermore, in cases where SelectiveNet was trained with a target coverage of $1.0$, its prediction head performed better than the CNN performance. In the asymmetrical setting, EC-SelectiveNet, trained with a target coverage of $1.0$, consistently outperformed all other methods in both binary and multi-class settings. This could be due to the selective part of the SelectiveNet loss function being ignored during training when a target of $1.0$ is used. Consequently, using only the predictive and auxiliary heads jointly to generate a loss, allows for a better representative encoder.

The utilisation of Bayesian neural networks had a negligible impact when averaging the predictions in any setting. However, leveraging the variance of the Monte Carlo samples led to superior results in the context of asymmetric settings and yielded promising outcomes on the $S_{unknown}$ dataset. Of the three methods for Bayesian neural networks, Laplace approximation exhibited the best performance, while Bayes by Backprop surprisingly performed well when using the variance. It can be speculated that the samples taken from the Bayes by Backprop were too similar, making the variance unsuitable for selective classification purposes. Notably, the use of different uncertainty measures led to varying results, with the predictive entropy measure of uncertainty surpassing all others, particularly in an asymmetric setting, while the variational ratios performed the worst in both symmetric and asymmetric cost settings.

The experiments reveal that the utilisation of temperature scaling for the calibration of predictions, which aims to enhance selective classification, resulted in elevated costs at higher coverage levels in asymmetrical cost settings. However, in other scenarios, the use of temperature scaling did not have any significant impact compared to the uncalibrated outcomes. The investigation encompasses diverse selective classification settings and underlines the necessity for further efforts to advance selective classification methods and comprehend their performance in asymmetrical cost settings. Such efforts will prove instrumental in the application of classification in clinical settings, where asymmetrical costs are prevalent and not all images can be classified, necessitating the use of rejection.

The study's findings highlight the importance of enhancing selective classification in order to simplify its adoption into clinical practise. This requirement is reinforced by \cite{jaeger2022call} observations, which reveal the limitations of neural networks in addressing failure detection, emphasising the constraints of selective categorization. A promising path forward is to tailor calibration approaches to specific decision-making scenarios. While still in its infancy, this study topic has potential~\citep{zhao2021calibrating} which introduces novel calibration methodologies for decision-specific scenarios. This new research represents the shifting landscape of selective classification and foreshadows transformational developments that could alter its path.