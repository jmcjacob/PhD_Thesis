\section{Introduction}
\label{sec:calibration_introduction}
This work was presented at the Uncertainty for Safe Utilization of Machine Learning in Medical Imaging (UNSURE) workshop hosted at Medical Image Computing and Computing Assisted Intervention 2022 (MICCAI) and published as part of the workshop proceedings \citep{carse2022calibration}.

Deep learning classifiers have been widely adopted for medical image analysis tasks such as diagnostic decision support, in certain restricted settings even outperforming medical specialists, e.g., binary classification of dermoscopic melanoma and nevi images~\citep{maron2019systematic}. Such promising results notwithstanding, further improvements are desirable to enable robust translation to the clinic. A known problem with deep neural network classifiers is that they can be poorly calibrated, typically resulting in over-confident predictions. Mis-calibration represents deviation between confidence and correctness. A model that is not well-calibrated cannot correctly identify the amount of uncertainty in its prediction. Well-calibrated probabilistic outputs are needed to support cost-sensitive and selective clinical decision-making~\citep{carse2021robust}, providing information helpful for avoiding adverse outcomes. Calibrated predictions are relevant for active learning~\citep{carse2019active}, reinforcement learning~\citep{dai2020federated} and out-of-distribution detection~\citep{ulmer2020trust}.

Several methods have been reported to improve the calibration of deep classifiers in medical image classification. However, their performance can be inconsistent and evidence for which to adopt for any given task and dataset is somewhat lacking. We present an empirical study that adds to this evidence base, focusing on classification datasets from dermatology and histopathology. 

Temperature scaling~\citep{guo2017calibration} is the most popular method for calibrating modern neural networks due to its post-hoc nature, simple implementation, and effectiveness. It works by scaling output logits using a temperature parameter that is optimized on a validation set. The original implementation called for optimizing temperature using negative log-likelihood. Other researchers have mentioned using calibration metrics to optimize temperature and have stated in passing that this led to improved calibration~\citep{mukhoti2020calibrating,frenkel2021network}. In this paper, we investigate this claim empirically using a density-based estimator for expected calibration error, and an estimator for maximum calibration error. Section~\ref{sec:calibration_measures} briefly discusses these calibration measures. We explore the effect of the use of different metrics for temperature optimization. We do this not only for neural networks trained by minimizing cross-entropy with one-hot encoded target labels but also for networks trained using focal loss and label smoothing. We also compare these with two Bayesian neural networks, based on Bayes-by-Backprop and a Laplace approximation method. These methods are described in Section~\ref{sec:calibration_review}. Details of our experiment design are provided in Section~\ref{sec:calibration_experiments}. Results on the ISIC 2019 multi-class dermatology dataset and the large Patch-Camelyon binary histopathology dataset are given in Section~\ref{subsec:calibration_results}, providing evidence on the relative performance of the methods in terms of calibration and accuracy on two contrasting tasks.



\section{Measures of Calibration}
\label{sec:calibration_measures}
Various metrics can be used to evaluate model calibration. The expected calibration error (ECE)~\citep{guo2017calibration} is a measure of the difference in expectation between the predictive confidence and classification accuracy. A commonly used estimator for ECE splits the probability interval into equally spaced bins. It then takes a weighted average of the absolute differences between the accuracy of the predictions in a bin and the mean of the probabilities in that bin. If we instead compute the maximum of this error over the bins, we obtain the maximum calibration error (MCE). This measure can be useful in high-risk situations where the worst-case calibration is relevant. 

While histogram-based estimation of ECE has proven useful for evaluating calibration~\citep{muller2019does}, others have criticized the use of equally spaced histogram binning because of inherent bias and statistical inefficiency. To address these criticisms, methods for adapting the number and sizes of bins have been proposed ~\citep{roelofs2022mitigating}. An alternative is to replace the use of histograms with continuous density estimators~\citep{zhang2020mix}. KDE-ECE uses kernel density estimation~\citep{parzen1962estimation} to estimate the densities of accuracy and confidence, achieving better data-efficiency. In this paper, we use KDE-ECE with a triweight kernel.

The negative log-likelihood is often utilized as a loss function for optimizing deep neural network classifiers, measuring a probabilistic model's quality by assessing its ability to recover the ground-truth conditional distribution. Negative log-likelihood can also be used as an indicator of a model's calibration.



\section{Calibration Methods and Literature Review}
\label{sec:calibration_review}
Methods that try to improve calibration can be roughly categorized into three broad approaches: regularization methods, post-processing methods, and methods designed to cope inherently with model uncertainty, such as Bayesian neural networks~\citep{gawlikowski2021survey}.


\subsection{Model Regularisation For Calibration}
Model regularization is performed by modifying the objective function used to optimize a model or by altering the dataset to encourage a model to be more robust and better calibrated. Dataset regularization methods have become commonplace with strategies such as data augmentation~\citep{hendrycks2019augmix} or adding out-of-distribution data to the training data~\citep{hendrycks2018deep}. We make use of data augmentation. In our experiments, we compare label smoothing and focal loss, two methods that can improve calibration.

Neural network classifiers are usually trained using a one-hot label encoding. This involves minimizing the expected cross-entropy between target outputs $t_k$ and the network outputs $y_k$, where $t_k=1$ for the true class and $t_k=0$ for the other classes. Label smoothing instead trains a network by minimizing the expected cross-entropy using modified targets $\hat{t} = (1 - \alpha) t + \frac{\alpha}{C}$ where $C$ is the number of classes and $\alpha$ is a free parameter~\citep{szegedy2016rethinking}. Label smoothing thus modifies the target class distribution and $\alpha$ controls the level of smoothing. At the extremes, $\alpha = 1$ results in a uniform distribution and $\alpha = 0$ recovers one-hot encoding. Label smoothing has been used in medical image analysis to improve calibration and robustness to out-of-distribution data~\citep{islam2021spatially}. Its popularity is partly due to ease of implementation and negligible computational overhead. 

Focal loss~\citep{lin2017focal} is a loss function that was originally introduced to improve the performance of object detection by encouraging a model to focus on samples with lower confidence. It has been shown to improve model calibration~\citep{mukhoti2020calibrating}. It weights predictions based on their confidence to encourage the model to learn more from examples closer to the decision boundary. This can reduce overconfidence, improve calibration, and lead to better results with unbalanced datasets. Focal loss can be written as $FL(y) = -\alpha(1-y)^\gamma\log(y)$. It adds the factor $(1-y)^\gamma$ to cross-entropy to weight the predictions. There are two hyper parameters: $\gamma>0$ determines the weighting of less confident examples, and $\alpha \in [0, 1]$ is used to improve the numerical stability.


\subsection{Post-Hoc Calibration}
Temperature scaling~\citep{guo2017calibration} is the most popular post-hoc calibration method for modern neural networks and has been used successfully in medical image analysis~\citep{liang2020improved}. It is a single-parameter version of Platt logistic scaling~\citep{platt1999probabilistic}. It applies a learned temperature parameter, $T > 0$, to rescale the output logits $z$ of a neural network before a softmax activation function is applied to output probabilistic predictions $\hat{y}$~(\ref{eq:temperature_scaling}). $T=1$ recovers the standard softmax activation. Since $T$ is used to scale all the logits, the output $\hat{y}$ has a monotonic relationship with the unscaled output. For this reason, classification accuracy is unaffected by temperature scaling.

\begin{equation}
	\hat{y} = \frac{e^{z/T}}{\sum^{J}_{j=1}e^{z_j/T}}
	\label{eq:temperature_scaling}
\end{equation}

The original implementation of temperature scaling optimizes $T$ by minimizing the negative log-likelihood of the predictions (expected cross-entropy) on a validation set. We have chosen to experiment with the use of alternative metrics for optimizing $T$. It seems reasonable to expect that optimizing $T$ for a calibration measure on a validation set will lead to better test calibration using that measure. Further details will be given in Section~\ref{sec:calibration_experiments}.


\subsection{Bayesian Approximation}
Bayesian neural networks infer distributions over their weight parameters rather than the usual point estimates. Monte Carlo sampling can then be used to approximate predictive distributions and obtain estimates of predictive means and uncertainty measures such as variance. Bayesian neural nets have been used in medical applications for improved calibration and uncertainty estimation~\citep{kwon2020uncertainty}.

Bayes by Backprop~\citep{blundell2015weight} trains a Bayesian neural network whilst using back-propagation to calculate gradients. It uses variational inference to approximate a posterior distribution over weights $q(w|\theta)$. The parameters for the distribution $\theta$ can be computed by minimizing the KL-Divergence between a variational posterior and the true posterior. As there is no way of knowing the true posterior, it is estimated using Monte Carlo sampling of the evidence lower bound. This is shown in Equation~(\ref{eq:elbo}) where $D$ is the dataset and $N$ is the number of Monte Carlo samples.

\begin{equation}
	\text{ELBO}(D, \theta) \approx \sum^N_{n=1}\log q(w^i|\theta)-\log p(w^i) - \log p(D|w^i)
	\label{eq:elbo}
\end{equation}

To train the Bayesian neural network the ELBO is used as part of a loss function alongside cross-entropy. The ELBO is weighted when combined with cross-entropy with the weighting of $\pi_m = \frac{2^{M-m}}{2^M-1}$ where $M$ is the number of batches and $m$ is the current batch. This weighting means that the first few batches are more influenced by the Bayesian complexity and the later batches learn more from the data. 

Laplace approximation~\citep{mackay1992bayesian} is a cost-effective method for training a Bayesian neural network by approximating the posterior as a Gaussian centered at a local maximum. This local maximum is obtained by training using a gradient-based optimiser in the usual way. Curvature is estimated using approximations to the Hessian matrix~\citep{botev2017practical} at the maximum. Thus, a Laplace approximation is fitted post-hoc onto a trained neural network so that probabilistic predictions can be sampled. Laplace approximation has become increasingly popular for Bayesian inference due to its ability to be added post-hoc and its low computational expense compared to other approximations such as Bayes by Backprop~\citep{daxberger2021laplace}.



\section{Calibration Experiments}
\label{sec:calibration_experiments}
This section details the datasets, training parameters, experimental setup, and results for the experiments with different methods for model calibration for medical image analysis. The code and full results used within this section can be found on the project GitHub repository\footnote{GitHub Repository: \url{github.com/UoD-CVIP/Medical_Calibration}}.

\subsection{Datasets}
Our experiments utilize two datasets, the ISIC 2019 challenge dataset~\citep{codella2018skin,combalia2019bcn20000,tschandl2018ham10000} and the Patch Camelyon (PCam) dataset~\citep{veeling2018rotation}. The ISIC 2019 dataset consists of 25,331 dermoscopic skin lesion images of 8 diagnostic classes: melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis, benign keratosis, dermatofibroma, vascular lesion, and squamous cell carcinoma. The PCam dataset is comprised of 327,680 96x96 pixel image patches extracted from whole-slide images of H\&E-stained lymph node sections from the Camelyon16 dataset~\citep{bejnordi2017diagnostic}. The datasets were split into training, validation and testing sets with proportions 6:2:2. The ISIC images were pre-processed by center-cropping their width to be equal to their height and then resizing to $256 \times 256$ pixels. Both datasets were augmented by normalizing each of the image channels, randomly flipping both horizontally and vertically, and randomly rotating by multiples of 90\textdegree.


\subsection{Experiment Setup}
We trained seven different types of CNN classifier on both the ISIC 2019 and the PCam datasets. In the case of ISIC 2019, each classifier was trained three times using different random seeds that altered the training, validation and testing data splits as well as the neural network weights initialization. The first classifier type, which we refer to as the baseline model, was trained using a standard cross-entropy function with one-hot label encoding. The next two classifiers used label smoothing (LS) with cross-entropy, with $\alpha$ set to $0.1$ and $0.2$, respectively. A further two classifiers types were trained using focal loss (FL) with $\gamma$ set to $2.0$ and $5.0$, respectively. We applied temperature scaling to each of these model types after training. In each case, we optimized the temperature parameter for various measures of calibration. These were the negative log-likelihood (nll), KDE-ECE (ece), and MCE (mce), as well as additive combinations of these three measures. We denote these using TS followed by an abbreviation for the measure used to optimize temperature, e.g., {\em TS nll+mce}. The temperature parameter was optimized using a Limited-memory BFGS optimizer~\citep{liu1989limited}. For comparison, two types of Bayesian neural networks were also trained. These used Bayes-by-Backprop and Laplace approximation, respectively.


\subsection{Training Parameters}
We used an EfficientNet~\citep{tan2019efficientnet} encoder with a compound coefficient of 7, pre-trained on ImageNet~\citep{deng2009imagenet} followed by a fully connected hidden layer with a width of 512 neurons before the output layer. For Bayesian convolutional neural networks, the final hidden and output fully-connected layers are replaced with Bayesian fully-connected layers that learn distributions for the weights and biases to be sampled from. All training used a cyclical learning rate scheduler with scheduling between $10^{-4}$ and $10^{-1}$. The batch size was 16 for the ISIC 2019 data and 64 for the PCam data. Bayes by backprop used a weighted loss function combining the expected lower bound from 10 forward-propagation passes and cross-entropy of the 10 predictions. The Laplace approximation was optimized post-hoc to a trained convolutional neural network model fitting the approximation on the output layer of the model using a full Hessian structure. Each model was trained for 40 epochs and the model with the best validation loss was used for evaluation.


\subsection{Results}
\label{subsec:calibration_results}

\begin{table}[h!]
	\centering
	\caption{Calibration and accuracy results for ISIC 2019 and PCam datasets. ISIC 2019 results are means and standard deviations over three iterations. Each section reports results from a single model type; TS denotes temperature scaling.}
	\label{tab:calibration_results}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{l|cccc|m{0.8cm}m{0.8cm}m{0.8cm}m{0.8cm}|}
			& \multicolumn{4}{c|}{ISIC 2019} & \multicolumn{4}{c|}{PCam} \\\cline{2-9} 
			& \multicolumn{1}{c|}{KDE-ECE} & \multicolumn{1}{c|}{MCE} & \multicolumn{1}{c|}{NLL} & ACC & \multicolumn{1}{m{0.9cm}|}{KDE-ECE} & \multicolumn{1}{m{0.9cm}|}{MCE} & \multicolumn{1}{m{0.9cm}|}{NLL} & \multicolumn{1}{m{0.9cm}|}{ACC} \\ \hline
			Baseline CNN & \multicolumn{1}{c|}{$0.046 \pm 0.017$} & \multicolumn{1}{c|}{$0.112 \pm 0.042$} & \multicolumn{1}{c|}{$0.514 \pm 0.015$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.187} & \multicolumn{1}{c|}{0.543} & \multicolumn{1}{c|}{0.848} \\
			TS nll & \multicolumn{1}{c|}{$0.015 \pm 0.002$} & \multicolumn{1}{c|}{$0.037 \pm 0.014$} & \multicolumn{1}{c|}{$0.471 \pm 0.017$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.187} & \multicolumn{1}{c|}{0.543} & \multicolumn{1}{c|}{0.848} \\
			TS ece & \multicolumn{1}{c|}{$0.017 \pm 0.007$} & \multicolumn{1}{c|}{$0.038 \pm 0.010$} & \multicolumn{1}{c|}{$0.472 \pm 0.017$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.204} & \multicolumn{1}{c|}{0.585} & \multicolumn{1}{c|}{0.848} \\
			TS mce & \multicolumn{1}{c|}{$0.016 \pm 0.004$} & \multicolumn{1}{c|}{$0.042 \pm 0.009$} & \multicolumn{1}{c|}{$0.471 \pm 0.018$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.197} & \multicolumn{1}{c|}{0.567} & \multicolumn{1}{c|}{0.848} \\
			TS nll+ece & \multicolumn{1}{c|}{$0.012 \pm 0.002$} & \multicolumn{1}{c|}{$0.043 \pm 0.012$} & \multicolumn{1}{c|}{$0.471 \pm 0.017$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.122} & \multicolumn{1}{c|}{0.210} & \multicolumn{1}{c|}{0.603} & \multicolumn{1}{c|}{0.848} \\
			TS nll+mce & \multicolumn{1}{c|}{$0.018 \pm 0.002$} & \multicolumn{1}{c|}{$0.038 \pm 0.010$} &  \multicolumn{1}{c|}{$0.472 \pm 0.018$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.210} & \multicolumn{1}{c|}{0.605} & \multicolumn{1}{c|}{0.848} \\
			TS ece+mce & \multicolumn{1}{c|}{$0.014 \pm 0.021$} & \multicolumn{1}{c|}{$0.041 \pm 0.014$} & \multicolumn{1}{c|}{$0.472 \pm 0.017$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.193} & \multicolumn{1}{c|}{0.557} & \multicolumn{1}{c|}{0.848} \\
			TS nll+ece+mce & \multicolumn{1}{c|}{$0.015 \pm 0.003$} & \multicolumn{1}{c|}{$0.042 \pm 0.011$} & \multicolumn{1}{c|}{$0.472 \pm 0.017$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.187} & \multicolumn{1}{c|}{0.543} & \multicolumn{1}{c|}{0.848} \\\hline \hline
			LS $\alpha=0.1$ & \multicolumn{1}{c|}{$0.039 \pm 0.001$} & \multicolumn{1}{c|}{$0.065 \pm 0.001$} & \multicolumn{1}{c|}{$0.495 \pm 0.011$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.116} & \multicolumn{1}{c|}{0.146} & \multicolumn{1}{c|}{0.398} & \multicolumn{1}{c|}{0.848} \\
			TS nll & \multicolumn{1}{c|}{$0.028 \pm 0.003$} & \multicolumn{1}{c|}{$0.087 \pm 0.021$} & \multicolumn{1}{c|}{$0.483 \pm 0.013$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.244} & \multicolumn{1}{c|}{0.536} & \multicolumn{1}{c|}{0.848} \\
			TS ece & \multicolumn{1}{c|}{$0.029 \pm 0.003$} & \multicolumn{1}{c|}{$0.134 \pm 0.023$} & \multicolumn{1}{c|}{$0.504 \pm 0.013$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.124} & \multicolumn{1}{c|}{0.346} & \multicolumn{1}{c|}{1.112} & \multicolumn{1}{c|}{0.848} \\
			TS mce & \multicolumn{1}{c|}{$0.053 \pm 0.016$} & \multicolumn{1}{c|}{$0.070 \pm 0.016$} & \multicolumn{1}{c|}{$0.505 \pm 0.004$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.121} & \multicolumn{1}{c|}{0.198} & \multicolumn{1}{c|}{0.423} & \multicolumn{1}{c|}{0.848} \\
			TS nll+ece & \multicolumn{1}{c|}{$0.028 \pm 0.003$} & \multicolumn{1}{c|}{$0.086 \pm 0.020$} & \multicolumn{1}{c|}{$0.048 \pm 0.013$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.237} & \multicolumn{1}{c|}{0.551} & \multicolumn{1}{c|}{0.848} \\
			TS nll+mce & \multicolumn{1}{c|}{$0.033 \pm 0.006$} & \multicolumn{1}{c|}{$0.071 \pm 0.016$} & \multicolumn{1}{c|}{$0.490 \pm 0.015$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.121} & \multicolumn{1}{c|}{0.198} & \multicolumn{1}{c|}{0.423} & \multicolumn{1}{c|}{0.848} \\
			TS ece+mce & \multicolumn{1}{c|}{$0.032 \pm 0.006$} & \multicolumn{1}{c|}{$0.072 \pm 0.016$} & \multicolumn{1}{c|}{$0.489 \pm 0.016$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.121} & \multicolumn{1}{c|}{0.198} & \multicolumn{1}{c|}{0.423} & \multicolumn{1}{c|}{0.848} \\
			TS nll+ece+mce & \multicolumn{1}{c|}{$0.031 \pm 0.004$} & \multicolumn{1}{c|}{$0.070 \pm 0.010$} & \multicolumn{1}{c|}{$0.488 \pm 0.014$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.121} & \multicolumn{1}{c|}{0.199} & \multicolumn{1}{c|}{0.424} & \multicolumn{1}{c|}{0.848} \\ \hline \hline
			LS $\alpha=0.2$ & \multicolumn{1}{c|}{$0.105 \pm 0.005$} & \multicolumn{1}{c|}{$0.173 \pm 0.050$} & \multicolumn{1}{c|}{$0.562 \pm 0.015$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.097} & \multicolumn{1}{c|}{0.142} & \multicolumn{1}{c|}{0.370} & \multicolumn{1}{c|}{0.852} \\
			TS nll & \multicolumn{1}{c|}{$0.037 \pm 0.001$} & \multicolumn{1}{c|}{$0.145 \pm 0.006$} & \multicolumn{1}{c|}{$0.503 \pm 0.013$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.118} & \multicolumn{1}{c|}{0.307} & \multicolumn{1}{c|}{0.601} & \multicolumn{1}{c|}{0.852} \\
			TS ece & \multicolumn{1}{c|}{$0.036 \pm 0.001$} & \multicolumn{1}{c|}{$0.134 \pm 0.023$} & \multicolumn{1}{c|}{$0.504 \pm 0.013$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.113} & \multicolumn{1}{c|}{0.226} & \multicolumn{1}{c|}{0.396} & \multicolumn{1}{c|}{0.852} \\
			TS mce & \multicolumn{1}{c|}{$0.095 \pm 0.018$} & \multicolumn{1}{c|}{$0.129 \pm 0.032$} & \multicolumn{1}{c|}{$0.553 \pm 0.015$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.113} & \multicolumn{1}{c|}{0.226} & \multicolumn{1}{c|}{0.396} & \multicolumn{1}{c|}{0.852} \\
			TS nll+ece & \multicolumn{1}{c|}{$0.036 \pm 0.001$} & \multicolumn{1}{c|}{$0.148 \pm 0.009$} & \multicolumn{1}{c|}{$0.503 \pm 0.013$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.117} & \multicolumn{1}{c|}{0.284} & \multicolumn{1}{c|}{0.508} & \multicolumn{1}{c|}{0.852} \\
			TS nll+mce & \multicolumn{1}{c|}{$0.051 \pm 0.006$} & \multicolumn{1}{c|}{$0.099 \pm 0.006$} & \multicolumn{1}{c|}{$0.514 \pm 0.012$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.113} & \multicolumn{1}{c|}{0.226} & \multicolumn{1}{c|}{0.396} & \multicolumn{1}{c|}{0.852} \\
			TS ece+mce & \multicolumn{1}{c|}{$0.047 \pm 0.003$} & \multicolumn{1}{c|}{$0.106 \pm 0.019$} & \multicolumn{1}{c|}{$0.511 \pm 0.011$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.113} & \multicolumn{1}{c|}{0.226} & \multicolumn{1}{c|}{0.396} & \multicolumn{1}{c|}{0.852} \\
			TS nll+ece+mce & \multicolumn{1}{c|}{$0.046 \pm 0.007$} & \multicolumn{1}{c|}{$0.113 \pm 0.012$} & \multicolumn{1}{c|}{$0.510 \pm 0.010$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.114} & \multicolumn{1}{c|}{0.243} & \multicolumn{1}{c|}{0.409} & \multicolumn{1}{c|}{0.852} \\ \hline \hline
			FL $\gamma=2.0$ & \multicolumn{1}{c|}{$0.057 \pm 0.020$}& \multicolumn{1}{c|}{$0.097 \pm 0.027$}& \multicolumn{1}{c|}{$0.491 \pm 0.013$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.122} & \multicolumn{1}{c|}{0.101} & \multicolumn{1}{c|}{0.356} & \multicolumn{1}{c|}{0.854} \\
			TS nll & \multicolumn{1}{c|}{$0.031 \pm 0.004$}& \multicolumn{1}{c|}{$0.078 \pm 0.018$}& \multicolumn{1}{c|}{$0.484 \pm 0.007$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.100} & \multicolumn{1}{c|}{0.155} & \multicolumn{1}{c|}{0.371} & \multicolumn{1}{c|}{0.854} \\
			TS ece & \multicolumn{1}{c|}{$0.011 \pm 0.003$}& \multicolumn{1}{c|}{$0.061 \pm 0.021$}& \multicolumn{1}{c|}{$0.492 \pm 0.004$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.101} & \multicolumn{1}{c|}{0.180} & \multicolumn{1}{c|}{0.388} & \multicolumn{1}{c|}{0.854} \\
			TS mce & \multicolumn{1}{c|}{$0.014 \pm 0.002$}& \multicolumn{1}{c|}{$0.062 \pm 0.021$}& \multicolumn{1}{c|}{$0.497 \pm 0.010$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.101} & \multicolumn{1}{c|}{0.180} & \multicolumn{1}{c|}{0.392} & \multicolumn{1}{c|}{0.854} \\
			TS nll+ece & \multicolumn{1}{c|}{$0.014 \pm 0.003$}& \multicolumn{1}{c|}{$0.062 \pm 0.018$}& \multicolumn{1}{c|}{$0.489 \pm 0.007$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.100} & \multicolumn{1}{c|}{0.170} & \multicolumn{1}{c|}{0.385} & \multicolumn{1}{c|}{0.854} \\
			TS nll+mce & \multicolumn{1}{c|}{$0.013 \pm 0.003$}& \multicolumn{1}{c|}{$0.063 \pm 0.018$}& \multicolumn{1}{c|}{$0.490 \pm 0.008$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.101} & \multicolumn{1}{c|}{0.180} & \multicolumn{1}{c|}{0.393} & \multicolumn{1}{c|}{0.854} \\
			TS ece+mce & \multicolumn{1}{c|}{$0.012 \pm 0.003$}& \multicolumn{1}{c|}{$0.062 \pm 0.021$}& \multicolumn{1}{c|}{$0.494 \pm 0.008$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.101} & \multicolumn{1}{c|}{0.179} & \multicolumn{1}{c|}{0.391} & \multicolumn{1}{c|}{0.854} \\
			TS nll+ece+mce & \multicolumn{1}{c|}{$0.026 \pm 0.020$}& \multicolumn{1}{c|}{$0.085 \pm 0.034$}& \multicolumn{1}{c|}{$0.497 \pm 0.013$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.101} & \multicolumn{1}{c|}{0.179} & \multicolumn{1}{c|}{0.392} & \multicolumn{1}{c|}{0.854} \\ \hline \hline
			FL $\gamma=5.0$ & \multicolumn{1}{c|}{$0.180 \pm 0.007$}& \multicolumn{1}{c|}{$0.250 \pm 0.010$}& \multicolumn{1}{c|}{$0.615 \pm 0.011$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.229} & \multicolumn{1}{c|}{0.289} & \multicolumn{1}{c|}{0.530} & \multicolumn{1}{c|}{0.835} \\
			TS nll & \multicolumn{1}{c|}{$0.061 \pm 0.010$}& \multicolumn{1}{c|}{$0.123 \pm 0.022$}& \multicolumn{1}{c|}{$0.551 \pm 0.011$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.068} & \multicolumn{1}{c|}{0.063} & \multicolumn{1}{c|}{0.382} & \multicolumn{1}{c|}{0.835} \\
			TS ece & \multicolumn{1}{c|}{$0.024 \pm 0.005$}& \multicolumn{1}{c|}{$0.084 \pm 0.026$}& \multicolumn{1}{c|}{$0.589 \pm 0.016$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.070} & \multicolumn{1}{c|}{0.077} & \multicolumn{1}{c|}{0.391} & \multicolumn{1}{c|}{0.835} \\
			TS mce & \multicolumn{1}{c|}{$0.031 \pm 0.009$}& \multicolumn{1}{c|}{$0.074 \pm 0.007$}& \multicolumn{1}{c|}{$0.621 \pm 0.046$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.387} & \multicolumn{1}{c|}{0.835} \\
			TS nll+ece & \multicolumn{1}{c|}{$0.032 \pm 0.008$}& \multicolumn{1}{c|}{$0.101 \pm 0.026$}& \multicolumn{1}{c|}{$0.561 \pm 0.011$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.071} & \multicolumn{1}{c|}{0.387} & \multicolumn{1}{c|}{0.835} \\
			TS nll+mce & \multicolumn{1}{c|}{$0.031 \pm 0.009$}& \multicolumn{1}{c|}{$0.102 \pm 0.024$}& \multicolumn{1}{c|}{$0.562 \pm 0.010$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.387} & \multicolumn{1}{c|}{0.835} \\
			TS ece+mce & \multicolumn{1}{c|}{$0.025 \pm 0.005$}& \multicolumn{1}{c|}{$0.079 \pm 0.012$}& \multicolumn{1}{c|}{$0.593 \pm 0.029$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.387} & \multicolumn{1}{c|}{0.835} \\
			TS nll+ece+mce & \multicolumn{1}{c|}{$0.028 \pm 0.007$}& \multicolumn{1}{c|}{$0.094 \pm 0.025$}& \multicolumn{1}{c|}{$0.569 \pm 0.009$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.387} & \multicolumn{1}{c|}{0.835} \\ \hline \hline
			Bayes-by-B'prop & \multicolumn{1}{c|}{$0.118 \pm 0.006$}& \multicolumn{1}{c|}{$0.260 \pm 0.021$}& \multicolumn{1}{c|}{$0.886 \pm 0.062$}& \multicolumn{1}{c|}{$0.795 \pm 0.069$}& \multicolumn{1}{c|}{0.115} & \multicolumn{1}{c|}{0.208} & \multicolumn{1}{c|}{0.551} & \multicolumn{1}{c|}{0.857} \\ 
			Laplace approx.& \multicolumn{1}{c|}{$0.041 \pm 0.016$}& \multicolumn{1}{c|}{$0.101 \pm 0.037$}& \multicolumn{1}{c|}{$0.507 \pm 0.010$}& \multicolumn{1}{c|}{$0.837 \pm 0.012$}& \multicolumn{1}{c|}{0.122} & \multicolumn{1}{c|}{0.210} & \multicolumn{1}{c|}{0.603} & \multicolumn{1}{c|}{0.848} \\\hline
		\end{tabular}%
	}
\end{table}

Table~\ref{tab:calibration_results} reports results for both the ISIC 2019 and the PCam datasets. It is divided into sections showing results for each trained model (with the temperature set to one) followed by results with temperature optimized for different measures of calibration. Note that temperature scaling will never affect accuracy. Each ISIC 2019 result is reported as a mean and standard deviation estimated from three runs.

On the multi-class skin lesion classification task, temperature scaling always improved calibration. The largest improvement in mean KDE-ECE when using the baseline CNN model was from 0.046 to 0.012; other temperature scaling measures achieved similar results. 

Label smoothing achieved better accuracy but resulted in calibration inferior to temperature scaling with one-hot labels. Adding temperature scaling to label smoothing tended to improve calibration but was not as effective as temperature scaling without label smoothing in this regard. Optimizing for MCE was ineffective. 

Focal loss ($\gamma = 2.0$) with temperature optimized for KDE-ECE achieved calibration and accuracy competitive with, and perhaps slightly better than, the cross-entropy model with temperature scaling. Both of the focal loss models showed behavior more akin to that reported in~\cite{mukhoti2020calibrating} in the sense that temperature optimized for KDE-ECE led to better calibration with a significant impact on KDE-ECE compared to temperature optimized for NLL.

On the binary classification task using the larger PCam dataset, we observe different behavior when using temperature scaling. The baseline CNN model gained no calibration benefit from temperature scaling, whichever calibration measure was used to optimize the temperature. 

Calibration using label smoothing appears to be better than that obtained by the baseline without temperature scaling. Adding temperature scaling to label smoothing appears to worsen calibration performance slightly on this dataset. We observed that calibration measures as functions of temperature had shallow minima; optimized temperature values obtained using the validation set differed from those that would have been optimal for the test set. 

For the LS model with $\alpha=0.2$ the model is encouraged to assign a prediction of 0.8 when confident about the prediction, while the accuracy of the model is around 80\% the predictions of the model will be better calibrated. 

Focal loss with $\gamma = 2.0$ had higher accuracy and similar calibration to the baseline without temperature scaling. However, unlike the baseline, both focal loss models' calibration was helped by temperature scaling. Focal loss ($\gamma = 5.0$) combined with temperature scaling achieved the best calibration of any of the models on the PCam dataset. Which measure was used to optimize temperature made little or no difference.

Bayes-by-Backprop performed relatively poorly on the ISIC 2019 dataset both in terms of calibration and accuracy. This could be due to the extra complexity involved with training a multi-class Bayesian neural network using back-propagation. On the other hand, Bayes-by-Backprop achieved the highest accuracy on the PCam dataset and had a lower KDE-ECE than the baseline network. Nevertheless, temperature scaling of networks trained with focal loss achieved comparable accuracy and better calibration in terms of both KDE-ECE and MCE. 

Laplace approximation outperformed Bayes-by-Backprop on ISIC 2019; it is also computationally less expensive. However, it was not competitive with temperature scaling in terms of calibration. On the larger PCam dataset, the Laplace approximation did not yield advantage over the baseline model.



\section{Conclusion}
\label{sec:calibration_conclusion}
This study has investigated deep neural network calibration for medical image classification and adds to the evidence base on the effectiveness of various calibration methods on two differing tasks and datasets. The results highlight the need to consider the choice of calibration method for each specific task. On ISIC2019, temperature scaling nearly always helped, giving a marked improvement in calibration with cross-entropy loss and focal loss trained networks. In contrast, temperature scaling did not improve calibration when used with cross-entropy trained networks on the PCam dataset, whereas it yielded improved calibration when combined with focal loss. While it is reasonable to expect that the use of calibration measures to tune temperature on validation data would improve calibration measured in that way, we have shown in our experimental results that this is not always the case. In most situations, the calibration performance is not significantly impacted by the measure used for optimization. We found that overall, focal loss combined with temperature scaling could achieve strong calibration performance. Further work could more fully explore the effect of hyperparameters such as $\gamma$ in this context.