\section{Introduction}
\label{sec:calibration_introduction}
Deep learning algorithms have been widely adopted for medical image analysis tasks and have even outperformed medical specialists in certain contexts, such as the binary classification of dermoscopic melanoma and nevi images~\citep{maron2019systematic}. However, to ensure reliable translation of these classifiers to clinical settings, further improvements are necessary. As discussed in previous chapters~\ref{ch:active_learning} and~\ref{ch:unsupervised_representation_learning}, a known challenge with deep learning algorithms is poor calibration, which often leads to overconfident predictions. Mis-calibration, or the deviation between confidence and correctness, can impair the model's ability to accurately identify uncertainty in its predictions. To support cost-sensitive and selective clinical decision-making (Chapter~\ref{ch:selective_classification}) and avoid adverse outcomes, well-calibrated probabilistic outputs are helpful. Calibrated predictions are also relevant for applications such as active learning (Chapter~\ref{ch:active_learning}), reinforcement learning~\citep{dai2020federated}, and out-of-distribution detection~\citep{ulmer2020trust}.

There have been numerous reports in the literature on techniques aimed at improving the calibration of deep learning classifiers in medical image classification. However, the effectiveness of these methods can be variable and there is a lack of clear guidance on which approach is most appropriate for a particular task or dataset. This chapter reports an empirical study which contributes to the existing body of evidence in this area by evaluating the performance of various calibration methods on medical image classification datasets from dermatology and histopathology.

Temperature scaling~\citep{guo2017calibration} is a widely used technique for calibrating modern neural networks, due in part to its post-hoc nature, ease of implementation, and demonstrated effectiveness. This method involves scaling output logits using a temperature parameter that is optimised on a validation set. The original implementation of temperature scaling utilised negative log-likelihood to optimise the temperature parameter. However, some researchers have proposed the use of other calibration metrics for this purpose, suggesting that this may lead to improved calibration~\citep{mukhoti2020calibrating,frenkel2021network} but omitting to provide empirical evidence to support. This chapter aims to evaluate this claim empirically, using density-based and maximum calibration error estimators as metrics for assessing the performance of temperature scaling with various optimisation approaches.

Section~\ref{sec:calibration_measures} describes various measures of calibration which have been proposed and studied in the literature. This chapter investigates the impact of using different metrics to optimise temperature in the context of deep learning classifiers for medical image analysis. The analysis includes networks trained using both traditional cross-entropy loss with one-hot encoded target labels, as well as more recently proposed methods such as focal loss and label smoothing.

In this chapter, the effectiveness of various calibration methods is also compared to that of two Bayesian neural network approaches: one based on Bayes-by-Backprop and the other utilising a Laplace approximation method. These methods are described in detail in Section~\ref{sec:calibration_review}. The experimental design and details of the study are provided in Section~\ref{sec:calibration_experiments}. The results of the analysis, which includes evaluations on the ISIC 2019 multi-class dermatology dataset~\citep{codella2018skin,combalia2019bcn20000,tschandl2018ham10000} and the large Patch-Camelyon binary histopathology dataset, are presented in Section~\ref{subsec:calibration_results}. These results provide insight into the relative performance of the different methods in terms of both calibration and accuracy on two distinct medical image classification tasks.

This work was presented at the Uncertainty for Safe Utilization of Machine Learning in Medical Imaging 2022 (UNSURE) workshop hosted at Medical Image Computing and Computing Assisted Intervention (MICCAI) in Singapore and published as part of its proceedings~\citep{carse2022calibration}.



\section{Measures of Calibration}
\label{sec:calibration_measures}
Reliability diagrams are graphical representations that compare the predicted probabilities from a model to the frequencies of the outcome variable. The ordinate of the reliability diagram depicts the empirical frequency of the outcome variable within a specific predicted probability bin, while the abscissa represents the predicted probability values for each bin. The optimal reliability diagram will exhibit a diagonal line, signifying that the predicted probabilities correspond precisely with the empirical frequencies. To construct a reliability diagram, The predicted probabilities are partitions into a set of equally sized probability bins. For instance, if there are 10 predicted probabilities ranging from 0 to 1 inclusive, these probabilities may be subdivided into 10 bins representing distinct ranges of predicted probabilities. For each bin the corresponding empirical frequency of occurrence each probability is plotted. Figure~\ref{fig:reliability_diagram} illustrates an example of a reliability diagram.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{images/reliability_diagram.png}
	\caption{Example reliability diagram.}
	\label{fig:reliability_diagram}
\end{figure}

There are several metrics that can be used to assess the calibration of a machine learning model. One such measure is the expected calibration error (ECE)~\citep{guo2017calibration}, which quantifies the discrepancy between predictive confidence and classification accuracy (Equation~(\ref{eq:ece})). A commonly used estimator for ECE involves dividing the range of predicted probabilities into a set of equally spaced bins and computing the weighted average of the absolute differences between the accuracy of the predictions in each bin and the mean of the probabilities in that bin. Another metric, known as the maximum calibration error (MCE), is obtained by taking the maximum of the error across all bins (Equation~(\ref{eq:mce})). This measure can be particularly useful in high-stakes situations where the worst-case calibration is of particular concern.

% TODO - Define symbols before referencing them.

\begin{equation}
	ECE=\sum_{m=1}^{M}\frac{\left|B_m\right|}{n}\left|\left(\frac{1}{\left|B_m\right|}\sum_{i\in B_m}1(\hat{y}_i=y_i)\right)-\left(\frac{1}{\left|B_m\right|}\sum_{i\in B_m}\hat{p}_i\right)\right|
	\label{eq:ece}
\end{equation}

\begin{equation}
	MCE = \max_m\left|\left(\frac{1}{\left|B_m\right|}\sum_{i\in B_m}1(\hat{y}_i=y_i)\right)-\left(\frac{1}{\left|B_m\right|}\sum_{i\in B_m}\hat{p}_i\right)\right|
	\label{eq:mce}
\end{equation}

Histogram-based estimation of ECE, which involves dividing the range of predicted probabilities into equally spaced bins, has been widely used to evaluate model calibration~\citep{muller2019does}. However, this approach has been criticised for its inherent bias and statistical inefficiency, leading to the development of methods that adapt the number and sizes of bins~\citep{roelofs2022mitigating}. An alternative approach is to use continuous density estimators, such as kernel density estimation (KDE)~\citep{parzen1962estimation}, to estimate the densities of accuracy and confidence in place of histograms. This method, known as KDE-ECE~\citep{zhang2020mix}, has been shown to offer improved data efficiency compared to histogram-based approaches. In this study, a triweight kernel was used when calculating KDE-ECE.

The negative log-likelihood loss function is a common choice for optimising deep neural network classifiers because it assesses the capacity of a probabilistic model to accurately predict the true conditional distribution. Beyond its use as an optimisation objective, the negative log-likelihood can also serve as a measure of a model's calibration.



\section{Review of Calibration Methods}
\label{sec:calibration_review}
There are several approaches that have been proposed to improve model calibration. These can be broadly grouped into three main categories: regularisation methods, post-processing methods, and methods that inherently account for model uncertainty, such as Bayesian neural networks~\citep{gawlikowski2021survey}. Regularisation methods aim to improve calibration by adding constraints or regularises to the model during training, while post-processing methods involve adjusting the model's predictions after training has been completed. Bayesian neural networks, on the other hand, are designed to explicitly incorporate uncertainty into the model, enabling more robust and well-calibrated predictions.

\subsection{Model Regularisation For Calibration}
Model regularisation aims to improve the generalisation and calibration of a machine learning model by modifying the objective function used to optimise the model or by altering the training data in a way that encourages the model to be more robust. Dataset regularisation methods, such as data augmentation~\citep{hendrycks2019augmix} or the inclusion of out-of-distribution data in the training set~\citep{hendrycks2018deep}, are widely used to achieve this goal. In this chapter, data augmentation is employed in all experiments, and the effects of two additional regularisation techniques, label smoothing and focal loss, on model calibration are also investigated.

Neural network classifiers are typically trained using one-hot label encoding, in which the objective is to minimise the expected cross-entropy between the target outputs $t_k$ and the network outputs $y_k$, where $t_k=1$ for the true class and $t_k=0$ for all other classes. An alternative approach, known as label smoothing, involves modifying the target class distribution by minimising the expected cross-entropy with modified targets $\hat{t} = (1 - \alpha) t + \frac{\alpha}{C}$, where $C$ is the number of classes and $\alpha$ is a free parameter~\citep{szegedy2016rethinking}. The parameter $\alpha$ controls the degree of smoothing, with $\alpha = 1$ resulting in a uniform distribution and $\alpha = 0$ corresponding to one-hot encoding. Label smoothing has been shown to improve calibration and robustness to out-of-distribution data in medical image analysis tasks~\citep{islam2021spatially} and is popular due to its ease of implementation and minimal computational overhead.

Focal loss is a loss function that was originally designed to improve the performance of object detection by encouraging the model to focus more on samples with lower confidence~\citep{lin2017focal}. It has been shown to improve model calibration in some cases~\citep{mukhoti2020calibrating}. Focal loss weights the predictions based on their confidence, with the goal of forcing the model to learn more from examples that are close to the decision boundary. This can reduce overconfidence, improve calibration, and lead to better performance on unbalanced datasets. Focal loss can be expressed as $FL(y) = -\alpha(1-y)^\gamma\log(y)$, where the factor $(1-y)^\gamma$ is included with the cross-entropy loss to weight the predictions. The hyperparameters $\gamma > 0$ and $\alpha \in [0, 1]$ control the weighting of less confident examples and improve numerical stability, respectively.


\subsection{Post-Hoc Calibration}
\label{subsec:post_hoc_calibration}
Temperature scaling is a widely used post-hoc calibration method for modern neural networks, including in medical image analysis applications~\citep{guo2017calibration,liang2020improved}. It is a single-parameter variant of Platt logistic scaling~\citep{platt1999probabilistic} that applies a learned temperature parameter, $T > 0$, to rescale the output logits $z$ of a neural network before applying a softmax activation function to obtain probabilistic predictions $\hat{y}$ (see Equation~(\ref{eq:temperature_scaling})). When $T=1$, the standard softmax activation is recovered. As the temperature parameter is used to scale all of the logits, the output $\hat{y}$ has a monotonic relationship with the unscaled output, meaning that classification accuracy is unaffected by temperature scaling.

\begin{equation}
	\hat{y} = \frac{e^{z/T}}{\sum^{J}_{j=1}e^{z_j/T}}
	\label{eq:temperature_scaling}
\end{equation}

The original implementation of temperature scaling optimises the temperature parameter $T$ by minimising the negative log-likelihood of the predictions (expected cross-entropy) on a validation set. In the experiments described in Section~\ref{sec:calibration_experiments}, alternative measures for optimising $T$ are considered. It is hypothesised that optimising $T$ using a calibration measure on a validation set will result in improved test calibration when evaluated using that same measure. Further details on the experimental design and results are provided in Section~\ref{sec:calibration_experiments}.


\subsection{Bayesian Approximation}
\label{subsec:bayesian_calibration}
Bayesian neural networks are a type of machine learning model that infer distributions over their weight parameters, as opposed to the traditional approach of obtaining point estimates. This allows for the use of Monte Carlo sampling to approximate predictive distributions and compute estimates of predictive means and uncertainty measures, such as variance. Bayesian neural networks have been applied in medical applications for the purpose of improving calibration and estimating uncertainty~\citep{kwon2020uncertainty}.

Bayes-by-Backprop~\citep{blundell2015weight} is a method for training Bayesian neural networks that combines the use of backpropagation to calculate gradients with variational inference to approximate the posterior distribution $q(w|\theta)$ over the model's weights. The parameters of this distribution, $\theta$, are determined by minimising the KL divergence between the variational posterior and the true posterior, which is estimated using Monte Carlo sampling of the evidence lower bound (ELBO) as shown in Equation~(\ref{eq:elbo}). Here $D$ is the dataset and $N$ is the number of Monte Carlo samples.

\begin{equation}
	\text{ELBO}(D, \theta) \approx \sum^N_{n=1}\log q(w^i|\theta)-\log p(w^i) - \log p(D|w^i)
	\label{eq:elbo}
\end{equation}

To train a Bayesian neural network, the ELBO is typically combined with the cross-entropy loss to form a composite loss function. When doing so, the ELBO is weighted by a factor of $\pi_m = \frac{2^{M-m}}{2^M-1}$, where $M$ is the total number of batches and $m$ is the current batch. This weighting scheme gives more influence on the Bayesian complexity term during the early stages of training, while allowing the model to learn more from the data as training progresses.

Laplace approximation~\citep{mackay1992bayesian} is a method to produce Bayesian neural networks from neural networks by approximating the posterior distribution over a model’s parameters by fitting a Gaussian distribution with a mean equal to the maximum a posteriori (MAP) of the parameter, and variance equal to the observed fisher information. The curvature of this Gaussian is estimated using approximations to the Hessian matrix at the maximum~\citep{botev2017practical}. Laplace approximation can be applied post-hoc to a trained neural network, allowing for the sampling of probabilistic predictions at low computational cost compared to other methods such as Bayes-by-Backprop~\citep{daxberger2021laplace}. This makes it a popular choice for Bayesian inference in practice.



\section{Calibration Experiments}
\label{sec:calibration_experiments}
This section presents the details of the datasets, training parameters, experimental setup, and results. The code and complete results for these experiments can be found in the project GitHub repository\footnote{GitHub Repository: \url{github.com/UoD-CVIP/Medical_Calibration}}.

\subsection{Datasets}
Two datasets were used: the ISIC 2019 challenge dataset~\citep{codella2018skin,combalia2019bcn20000,tschandl2018ham10000} and the Patch Camelyon (PCam) dataset~\citep{veeling2018rotation}. The ISIC 2019 dataset consists of 25,331 dermoscopic skin lesion images belonging to eight diagnostic classes: melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis, benign keratosis, dermatofibroma, vascular lesion, and squamous cell carcinoma (example images of each classification in Figure~\ref{fig:ISIC_examples}). The PCam dataset consists of 327,680 96x96 pixel image patches extracted from whole-slide images of H\&E-stained lymph node sections from the Camelyon16 dataset~\citep{bejnordi2017diagnostic} (example images in Figure~\ref{fig:pcam_examples}). The datasets were split into training, validation, and testing sets with proportions 6:2:2. As the ISIC 2019 images vary in size each image was pre-processed by cropping their width to be equal to their height and resizing to $256 \times 256$ pixels. Both datasets were augmented through normalisation of each image channel, random horizontal and vertical flipping, and random rotation by multiples of 90\textdegree.

\begin{figure}[!h]
	\centering
	\captionsetup[subfigure]{singlelinecheck=false}
	\begin{tabular}{cc}
		\subcaptionbox{\centering Melanoma}{\includegraphics[width=0.4\textwidth]{images/MEL.jpg}} &
		\subcaptionbox{\centering Melanocytic \mbox{Nevus}}{\includegraphics[width=0.4\textwidth]{images/nv_example.jpeg}} \\
		\subcaptionbox{\centering Basal Cell \mbox{Carcinoma}}{\includegraphics[width=0.4\textwidth]{images/bcc_example.jpeg}} &
		\subcaptionbox{\centering Actinic Keratosis}{\includegraphics[width=0.4\textwidth]{images/ak_example.jpeg}} \\
		\subcaptionbox{\centering Benign Keratosis}{\includegraphics[width=0.4\textwidth]{images/bkl_example.jpeg}} &
		\subcaptionbox{\centering Dermatofibroma}{\includegraphics[width=0.4\textwidth]{images/df_example.jpeg}} \\
		\subcaptionbox{\centering Vascular Lesion}{\includegraphics[width=0.4\textwidth]{images/vasc_example.jpeg}} &
		\subcaptionbox{\centering Squamous Cell Carcinoma}{\includegraphics[width=0.4\textwidth]{images/scc_example.jpeg}}
	\end{tabular}
	\caption{Example images from the ISIC Challenge 2019 dataset~\citep{codella2018skin,combalia2019bcn20000,tschandl2018ham10000}.}
	\label{fig:ISIC_examples}
\end{figure}

\subsection{Experiment Setup}
Seven different types of CNN classifiers were trained on both the ISIC 2019 and the PCam datasets. In the case of ISIC 2019, each classifier was trained three times using different random seeds, which altered the data splits and weights initialisations for the training, validation, and testing sets. The first classifier, referred to as the baseline model, was trained using a standard cross-entropy function with one-hot label encoding. The next two classifiers used label smoothing with cross-entropy, with alpha values of 0.1 and 0.2, respectively. An additional two classifiers were trained using focal loss, with gamma values of 2.0 and 5.0, respectively. Temperature scaling was applied to each of these model types after training, with the temperature parameter optimised using a Limited-memory BFGS optimiser~\citep{liu1989limited} for various measures of calibration, including the negative log-likelihood, KDE-ECE, MCE, and combinations of these three measures. For comparison, two types of Bayesian neural networks were also trained, using Bayes-by-Backprop and Laplace approximation, respectively.

\subsection{Training Parameters}
An EfficientNet encoder with a compound coefficient of 7, pre-trained on ImageNet, was utilised as the CNN model in this study. The encoder was followed by a fully connected hidden layer with a width of 512 neurons before the output layer. For Bayesian convolutional neural networks, the final hidden and output fully-connected layers were replaced with Bayesian fully-connected layers that learn distributions for the weights and biases to be sampled from. Cyclical learning rate scheduling was employed, with scheduling between $10^{-4}$ and $10^{-1}$. The batch size for the ISIC 2019 data was 16, and the batch size for the PCam data was 64. Bayes-by-backprop used a weighted loss function that combined the expected lower bound from 10 forward-propagation passes and the cross-entropy of the 10 predictions. The Laplace approximation was optimised post-hoc by fitting it to a trained convolutional neural network model on the output layer of the model using a full Hessian structure. Each model was trained for 40 epochs, and the model with the best validation loss was selected for evaluation.

\subsection{Results}
\label{subsec:calibration_results}

% TODO - Present this as a curve (as in the presentation).
% TODO - Check for statistical significence
% TODO - Split up table
% TODO - Indicate if a metric should be higher or lower.

\begin{table}[h!]
	\centering
	\caption{Calibration and accuracy results for ISIC 2019 and PCam datasets. ISIC 2019 results are means and standard deviations over three iterations. Each section reports results from a single model type; TS denotes temperature scaling.}
	\label{tab:calibration_results}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{l|cccc|m{0.8cm}m{0.8cm}m{0.8cm}m{0.8cm}|}
			& \multicolumn{4}{c|}{ISIC 2019} & \multicolumn{4}{c|}{PCam} \\\cline{2-9} 
			& \multicolumn{1}{c|}{KDE-ECE} & \multicolumn{1}{c|}{MCE} & \multicolumn{1}{c|}{NLL} & ACC & \multicolumn{1}{m{0.9cm}|}{KDE-ECE} & \multicolumn{1}{m{0.9cm}|}{MCE} & \multicolumn{1}{m{0.9cm}|}{NLL} & \multicolumn{1}{m{0.9cm}|}{ACC} \\ \hline
			Baseline CNN & \multicolumn{1}{c|}{$0.046 \pm 0.017$} & \multicolumn{1}{c|}{$0.112 \pm 0.042$} & \multicolumn{1}{c|}{$0.514 \pm 0.015$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.187} & \multicolumn{1}{c|}{0.543} & \multicolumn{1}{c|}{0.848} \\
			TS nll & \multicolumn{1}{c|}{$0.015 \pm 0.002$} & \multicolumn{1}{c|}{$0.037 \pm 0.014$} & \multicolumn{1}{c|}{$0.471 \pm 0.017$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.187} & \multicolumn{1}{c|}{0.543} & \multicolumn{1}{c|}{0.848} \\
			TS ece & \multicolumn{1}{c|}{$0.017 \pm 0.007$} & \multicolumn{1}{c|}{$0.038 \pm 0.010$} & \multicolumn{1}{c|}{$0.472 \pm 0.017$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.204} & \multicolumn{1}{c|}{0.585} & \multicolumn{1}{c|}{0.848} \\
			TS mce & \multicolumn{1}{c|}{$0.016 \pm 0.004$} & \multicolumn{1}{c|}{$0.042 \pm 0.009$} & \multicolumn{1}{c|}{$0.471 \pm 0.018$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.197} & \multicolumn{1}{c|}{0.567} & \multicolumn{1}{c|}{0.848} \\
			TS nll+ece & \multicolumn{1}{c|}{$0.012 \pm 0.002$} & \multicolumn{1}{c|}{$0.043 \pm 0.012$} & \multicolumn{1}{c|}{$0.471 \pm 0.017$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.122} & \multicolumn{1}{c|}{0.210} & \multicolumn{1}{c|}{0.603} & \multicolumn{1}{c|}{0.848} \\
			TS nll+mce & \multicolumn{1}{c|}{$0.018 \pm 0.002$} & \multicolumn{1}{c|}{$0.038 \pm 0.010$} &  \multicolumn{1}{c|}{$0.472 \pm 0.018$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.210} & \multicolumn{1}{c|}{0.605} & \multicolumn{1}{c|}{0.848} \\
			TS ece+mce & \multicolumn{1}{c|}{$0.014 \pm 0.021$} & \multicolumn{1}{c|}{$0.041 \pm 0.014$} & \multicolumn{1}{c|}{$0.472 \pm 0.017$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.193} & \multicolumn{1}{c|}{0.557} & \multicolumn{1}{c|}{0.848} \\
			TS nll+ece+mce & \multicolumn{1}{c|}{$0.015 \pm 0.003$} & \multicolumn{1}{c|}{$0.042 \pm 0.011$} & \multicolumn{1}{c|}{$0.472 \pm 0.017$} & \multicolumn{1}{c|}{$0.836 \pm 0.012$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.187} & \multicolumn{1}{c|}{0.543} & \multicolumn{1}{c|}{0.848} \\\hline \hline
			LS $\alpha=0.1$ & \multicolumn{1}{c|}{$0.039 \pm 0.001$} & \multicolumn{1}{c|}{$0.065 \pm 0.001$} & \multicolumn{1}{c|}{$0.495 \pm 0.011$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.116} & \multicolumn{1}{c|}{0.146} & \multicolumn{1}{c|}{0.398} & \multicolumn{1}{c|}{0.848} \\
			TS nll & \multicolumn{1}{c|}{$0.028 \pm 0.003$} & \multicolumn{1}{c|}{$0.087 \pm 0.021$} & \multicolumn{1}{c|}{$0.483 \pm 0.013$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.244} & \multicolumn{1}{c|}{0.536} & \multicolumn{1}{c|}{0.848} \\
			TS ece & \multicolumn{1}{c|}{$0.029 \pm 0.003$} & \multicolumn{1}{c|}{$0.134 \pm 0.023$} & \multicolumn{1}{c|}{$0.504 \pm 0.013$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.124} & \multicolumn{1}{c|}{0.346} & \multicolumn{1}{c|}{1.112} & \multicolumn{1}{c|}{0.848} \\
			TS mce & \multicolumn{1}{c|}{$0.053 \pm 0.016$} & \multicolumn{1}{c|}{$0.070 \pm 0.016$} & \multicolumn{1}{c|}{$0.505 \pm 0.004$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.121} & \multicolumn{1}{c|}{0.198} & \multicolumn{1}{c|}{0.423} & \multicolumn{1}{c|}{0.848} \\
			TS nll+ece & \multicolumn{1}{c|}{$0.028 \pm 0.003$} & \multicolumn{1}{c|}{$0.086 \pm 0.020$} & \multicolumn{1}{c|}{$0.048 \pm 0.013$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.123} & \multicolumn{1}{c|}{0.237} & \multicolumn{1}{c|}{0.551} & \multicolumn{1}{c|}{0.848} \\
			TS nll+mce & \multicolumn{1}{c|}{$0.033 \pm 0.006$} & \multicolumn{1}{c|}{$0.071 \pm 0.016$} & \multicolumn{1}{c|}{$0.490 \pm 0.015$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.121} & \multicolumn{1}{c|}{0.198} & \multicolumn{1}{c|}{0.423} & \multicolumn{1}{c|}{0.848} \\
			TS ece+mce & \multicolumn{1}{c|}{$0.032 \pm 0.006$} & \multicolumn{1}{c|}{$0.072 \pm 0.016$} & \multicolumn{1}{c|}{$0.489 \pm 0.016$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.121} & \multicolumn{1}{c|}{0.198} & \multicolumn{1}{c|}{0.423} & \multicolumn{1}{c|}{0.848} \\
			TS nll+ece+mce & \multicolumn{1}{c|}{$0.031 \pm 0.004$} & \multicolumn{1}{c|}{$0.070 \pm 0.010$} & \multicolumn{1}{c|}{$0.488 \pm 0.014$} & \multicolumn{1}{c|}{$0.855 \pm 0.005$} & \multicolumn{1}{c|}{0.121} & \multicolumn{1}{c|}{0.199} & \multicolumn{1}{c|}{0.424} & \multicolumn{1}{c|}{0.848} \\ \hline \hline
			LS $\alpha=0.2$ & \multicolumn{1}{c|}{$0.105 \pm 0.005$} & \multicolumn{1}{c|}{$0.173 \pm 0.050$} & \multicolumn{1}{c|}{$0.562 \pm 0.015$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.097} & \multicolumn{1}{c|}{0.142} & \multicolumn{1}{c|}{0.370} & \multicolumn{1}{c|}{0.852} \\
			TS nll & \multicolumn{1}{c|}{$0.037 \pm 0.001$} & \multicolumn{1}{c|}{$0.145 \pm 0.006$} & \multicolumn{1}{c|}{$0.503 \pm 0.013$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.118} & \multicolumn{1}{c|}{0.307} & \multicolumn{1}{c|}{0.601} & \multicolumn{1}{c|}{0.852} \\
			TS ece & \multicolumn{1}{c|}{$0.036 \pm 0.001$} & \multicolumn{1}{c|}{$0.134 \pm 0.023$} & \multicolumn{1}{c|}{$0.504 \pm 0.013$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.113} & \multicolumn{1}{c|}{0.226} & \multicolumn{1}{c|}{0.396} & \multicolumn{1}{c|}{0.852} \\
			TS mce & \multicolumn{1}{c|}{$0.095 \pm 0.018$} & \multicolumn{1}{c|}{$0.129 \pm 0.032$} & \multicolumn{1}{c|}{$0.553 \pm 0.015$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.113} & \multicolumn{1}{c|}{0.226} & \multicolumn{1}{c|}{0.396} & \multicolumn{1}{c|}{0.852} \\
			TS nll+ece & \multicolumn{1}{c|}{$0.036 \pm 0.001$} & \multicolumn{1}{c|}{$0.148 \pm 0.009$} & \multicolumn{1}{c|}{$0.503 \pm 0.013$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.117} & \multicolumn{1}{c|}{0.284} & \multicolumn{1}{c|}{0.508} & \multicolumn{1}{c|}{0.852} \\
			TS nll+mce & \multicolumn{1}{c|}{$0.051 \pm 0.006$} & \multicolumn{1}{c|}{$0.099 \pm 0.006$} & \multicolumn{1}{c|}{$0.514 \pm 0.012$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.113} & \multicolumn{1}{c|}{0.226} & \multicolumn{1}{c|}{0.396} & \multicolumn{1}{c|}{0.852} \\
			TS ece+mce & \multicolumn{1}{c|}{$0.047 \pm 0.003$} & \multicolumn{1}{c|}{$0.106 \pm 0.019$} & \multicolumn{1}{c|}{$0.511 \pm 0.011$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.113} & \multicolumn{1}{c|}{0.226} & \multicolumn{1}{c|}{0.396} & \multicolumn{1}{c|}{0.852} \\
			TS nll+ece+mce & \multicolumn{1}{c|}{$0.046 \pm 0.007$} & \multicolumn{1}{c|}{$0.113 \pm 0.012$} & \multicolumn{1}{c|}{$0.510 \pm 0.010$} & \multicolumn{1}{c|}{$0.862 \pm 0.009$} & \multicolumn{1}{c|}{0.114} & \multicolumn{1}{c|}{0.243} & \multicolumn{1}{c|}{0.409} & \multicolumn{1}{c|}{0.852} \\ \hline \hline
			FL $\gamma=2.0$ & \multicolumn{1}{c|}{$0.057 \pm 0.020$}& \multicolumn{1}{c|}{$0.097 \pm 0.027$}& \multicolumn{1}{c|}{$0.491 \pm 0.013$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.122} & \multicolumn{1}{c|}{0.101} & \multicolumn{1}{c|}{0.356} & \multicolumn{1}{c|}{0.854} \\
			TS nll & \multicolumn{1}{c|}{$0.031 \pm 0.004$}& \multicolumn{1}{c|}{$0.078 \pm 0.018$}& \multicolumn{1}{c|}{$0.484 \pm 0.007$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.100} & \multicolumn{1}{c|}{0.155} & \multicolumn{1}{c|}{0.371} & \multicolumn{1}{c|}{0.854} \\
			TS ece & \multicolumn{1}{c|}{$0.011 \pm 0.003$}& \multicolumn{1}{c|}{$0.061 \pm 0.021$}& \multicolumn{1}{c|}{$0.492 \pm 0.004$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.101} & \multicolumn{1}{c|}{0.180} & \multicolumn{1}{c|}{0.388} & \multicolumn{1}{c|}{0.854} \\
			TS mce & \multicolumn{1}{c|}{$0.014 \pm 0.002$}& \multicolumn{1}{c|}{$0.062 \pm 0.021$}& \multicolumn{1}{c|}{$0.497 \pm 0.010$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.101} & \multicolumn{1}{c|}{0.180} & \multicolumn{1}{c|}{0.392} & \multicolumn{1}{c|}{0.854} \\
			TS nll+ece & \multicolumn{1}{c|}{$0.014 \pm 0.003$}& \multicolumn{1}{c|}{$0.062 \pm 0.018$}& \multicolumn{1}{c|}{$0.489 \pm 0.007$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.100} & \multicolumn{1}{c|}{0.170} & \multicolumn{1}{c|}{0.385} & \multicolumn{1}{c|}{0.854} \\
			TS nll+mce & \multicolumn{1}{c|}{$0.013 \pm 0.003$}& \multicolumn{1}{c|}{$0.063 \pm 0.018$}& \multicolumn{1}{c|}{$0.490 \pm 0.008$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.101} & \multicolumn{1}{c|}{0.180} & \multicolumn{1}{c|}{0.393} & \multicolumn{1}{c|}{0.854} \\
			TS ece+mce & \multicolumn{1}{c|}{$0.012 \pm 0.003$}& \multicolumn{1}{c|}{$0.062 \pm 0.021$}& \multicolumn{1}{c|}{$0.494 \pm 0.008$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.101} & \multicolumn{1}{c|}{0.179} & \multicolumn{1}{c|}{0.391} & \multicolumn{1}{c|}{0.854} \\
			TS nll+ece+mce & \multicolumn{1}{c|}{$0.026 \pm 0.020$}& \multicolumn{1}{c|}{$0.085 \pm 0.034$}& \multicolumn{1}{c|}{$0.497 \pm 0.013$}& \multicolumn{1}{c|}{$0.840 \pm 0.004$}& \multicolumn{1}{c|}{0.101} & \multicolumn{1}{c|}{0.179} & \multicolumn{1}{c|}{0.392} & \multicolumn{1}{c|}{0.854} \\ \hline \hline
			FL $\gamma=5.0$ & \multicolumn{1}{c|}{$0.180 \pm 0.007$}& \multicolumn{1}{c|}{$0.250 \pm 0.010$}& \multicolumn{1}{c|}{$0.615 \pm 0.011$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.229} & \multicolumn{1}{c|}{0.289} & \multicolumn{1}{c|}{0.530} & \multicolumn{1}{c|}{0.835} \\
			TS nll & \multicolumn{1}{c|}{$0.061 \pm 0.010$}& \multicolumn{1}{c|}{$0.123 \pm 0.022$}& \multicolumn{1}{c|}{$0.551 \pm 0.011$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.068} & \multicolumn{1}{c|}{0.063} & \multicolumn{1}{c|}{0.382} & \multicolumn{1}{c|}{0.835} \\
			TS ece & \multicolumn{1}{c|}{$0.024 \pm 0.005$}& \multicolumn{1}{c|}{$0.084 \pm 0.026$}& \multicolumn{1}{c|}{$0.589 \pm 0.016$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.070} & \multicolumn{1}{c|}{0.077} & \multicolumn{1}{c|}{0.391} & \multicolumn{1}{c|}{0.835} \\
			TS mce & \multicolumn{1}{c|}{$0.031 \pm 0.009$}& \multicolumn{1}{c|}{$0.074 \pm 0.007$}& \multicolumn{1}{c|}{$0.621 \pm 0.046$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.387} & \multicolumn{1}{c|}{0.835} \\
			TS nll+ece & \multicolumn{1}{c|}{$0.032 \pm 0.008$}& \multicolumn{1}{c|}{$0.101 \pm 0.026$}& \multicolumn{1}{c|}{$0.561 \pm 0.011$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.071} & \multicolumn{1}{c|}{0.387} & \multicolumn{1}{c|}{0.835} \\
			TS nll+mce & \multicolumn{1}{c|}{$0.031 \pm 0.009$}& \multicolumn{1}{c|}{$0.102 \pm 0.024$}& \multicolumn{1}{c|}{$0.562 \pm 0.010$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.387} & \multicolumn{1}{c|}{0.835} \\
			TS ece+mce & \multicolumn{1}{c|}{$0.025 \pm 0.005$}& \multicolumn{1}{c|}{$0.079 \pm 0.012$}& \multicolumn{1}{c|}{$0.593 \pm 0.029$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.387} & \multicolumn{1}{c|}{0.835} \\
			TS nll+ece+mce & \multicolumn{1}{c|}{$0.028 \pm 0.007$}& \multicolumn{1}{c|}{$0.094 \pm 0.025$}& \multicolumn{1}{c|}{$0.569 \pm 0.009$}& \multicolumn{1}{c|}{$0.823 \pm 0.002$}& \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.069} & \multicolumn{1}{c|}{0.387} & \multicolumn{1}{c|}{0.835} \\ \hline \hline
			Bayes-by-B'prop & \multicolumn{1}{c|}{$0.118 \pm 0.006$}& \multicolumn{1}{c|}{$0.260 \pm 0.021$}& \multicolumn{1}{c|}{$0.886 \pm 0.062$}& \multicolumn{1}{c|}{$0.795 \pm 0.069$}& \multicolumn{1}{c|}{0.115} & \multicolumn{1}{c|}{0.208} & \multicolumn{1}{c|}{0.551} & \multicolumn{1}{c|}{0.857} \\ 
			Laplace approx.& \multicolumn{1}{c|}{$0.041 \pm 0.016$}& \multicolumn{1}{c|}{$0.101 \pm 0.037$}& \multicolumn{1}{c|}{$0.507 \pm 0.010$}& \multicolumn{1}{c|}{$0.837 \pm 0.012$}& \multicolumn{1}{c|}{0.122} & \multicolumn{1}{c|}{0.210} & \multicolumn{1}{c|}{0.603} & \multicolumn{1}{c|}{0.848} \\\hline
		\end{tabular}%
	}
\end{table}

The results for the ISIC 2019 and PCam datasets are presented in Table~\ref{tab:calibration_results}. The table is divided into sections showing the results for each trained model with the temperature set to one, followed by the results with the temperature optimised for various measures of calibration. It is important to note that temperature scaling does not affect accuracy. The results for the ISIC 2019 dataset are reported as the mean and standard deviation, estimated from three runs.

On the multi-class skin lesion classification task, temperature scaling consistently improved calibration. The largest improvement in mean KDE-ECE when using the baseline CNN model was from 0.046 to 0.012; other temperature scaling measures yielded similar results. Label smoothing achieved better accuracy, but the calibration was inferior to temperature scaling with one-hot labels. When temperature scaling was added to label smoothing, it tended to improve calibration, but it was not as effective as temperature scaling without label smoothing in this regard. Optimising for MCE was ineffective.

Focal loss with a value of $\gamma = 2.0$ and temperature optimised for KDE-ECE achieved calibration and accuracy that were competitive with, or perhaps slightly better than, the cross-entropy model with temperature scaling. Both of the focal loss models showed behaviour similar to that reported in~\cite{mukhoti2020calibrating}, in that temperature optimisation for KDE-ECE resulted in better calibration with a significant impact on KDE-ECE compared to temperature optimisation for NLL.

On the binary classification task using the PCam dataset, a different behaviour was observed when using temperature scaling. The baseline CNN model did not benefit from temperature scaling in terms of calibration, regardless of the calibration measure used to optimise the temperature.

Calibration using label smoothing appears to be better than that obtained by the baseline model without temperature scaling. Adding temperature scaling to label smoothing appears to slightly worsen the calibration performance on this dataset. It can be observed that the calibration measures as functions of temperature had shallow minima, and the optimised temperature values obtained using the validation set differed from those that would have been optimal for the test set.

For the LS model with $\alpha=0.2$, the model is encouraged to assign a prediction of 0.8 when it is confident about the prediction. While the accuracy of the model is around 80\%, the predictions of the model will be better calibrated.

Focal loss with $\gamma = 2.0$ had higher accuracy and similar calibration to the baseline model without temperature scaling. However, unlike the baseline, both focal loss models benefited from temperature scaling in terms of calibration. Focal loss with $\gamma = 5.0$ and temperature scaling achieved the best calibration of any of the models on the PCam dataset. The measure used to optimise the temperature made little or no difference in the results.

Bayes-by-Backprop performed relatively poorly on the ISIC 2019 dataset in terms of both calibration and accuracy. This could be due to the additional complexity involved with training a multi-class Bayesian neural network using back-propagation. On the other hand, Bayes-by-Backprop achieved the highest accuracy on the PCam dataset and had a lower KDE-ECE than the baseline network. Nevertheless, temperature scaling of networks trained with focal loss achieved comparable accuracy and better calibration in terms of both KDE-ECE and MCE.

The Laplace approximation outperformed Bayes-by-Backprop on the ISIC 2019 dataset and is also computationally less expensive. However, it was not competitive with temperature scaling in terms of calibration. On the larger PCam dataset, the Laplace approximation did not provide an advantage over the baseline model.



\section{Conclusion}
\label{sec:calibration_conclusion}

% TODO - Discuss theoretical considerations
% TODO - Suggest how to improve exisiting methods

In this chapter, deep neural network calibration for medical image classification was investigated and added to the existing evidence on the effectiveness of various calibration methods on two different tasks and datasets. The results of this study demonstrate the need to consider the choice of calibration method for each specific task. On the ISIC2019 dataset, temperature scaling nearly always improved calibration, giving a marked improvement in calibration with cross-entropy loss and focal loss trained networks. In contrast, temperature scaling did not improve calibration when used with cross-entropy trained networks on the PCam dataset, but it did yield improved calibration when combined with focal loss.

While it is reasonable to expect that the use of calibration measures to tune temperature on validation data would improve calibration measured in that way, the experimental results of this study showed that this is not always the case. In most situations, the calibration performance is not significantly impacted by the measure used for optimisation. Overall, focal loss combined with temperature scaling appeared to be a strong approach for achieving good calibration performance. Further research could more fully explore the effect of hyperparameters such as $\gamma$ in this context.